{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaf63d8b",
   "metadata": {},
   "source": [
    "### 1. Explain succinctly in your own words (but working with a ChatBot if needed)...<br>\n",
    "\n",
    "1. the difference between **Simple Linear Regression** and **Multiple Linear Regression**; and the benefit the latter provides over the former\n",
    "\n",
    "\n",
    "2. the difference between using a **continuous variable** and an **indicator variable** in **Simple Linear Regression**; and these two **linear forms**\n",
    "\n",
    "\n",
    "3. the change that happens in the behavior of the model (i.e., the expected nature of the data it models) when a single **indicator variable** is introduced alongside a **continuous variable** to create a **Multiple Linear Regression**; and these two **linear forms** (i.e., the **Simple Linear Regression** versus the **Multiple Linear Regression**)\n",
    "\n",
    "\n",
    "4. the effect of adding an **interaction** between a **continuous** and an **indicator variable** in **Multiple Linear Regression** models; and this **linear form**\n",
    "\n",
    "\n",
    "5. the behavior of a **Multiple Linear Regression** model (i.e., the expected nature of the data it models) based only on **indicator variables** derived from a **non-binary categorical variable**; this **linear form**; and the necessarily resulting **binary variable encodings** it utilizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db08a25a",
   "metadata": {},
   "source": [
    "1. In simple linear regression, we compare the correlation between 1 dependent and 1 independent variable. In multiple linear regression, we compare the correlation of 1 dependent and multiple independent variable. Often, we say there is a primary independent variable and the rest would be covariates. The primary predictor (independent variable) is what we're interested in studying and the covariates are additional variables that might account for the influence of the outcome variable. In a simple regression we can only see the correlation of 1 predictor to an outcome, but multiple regression allows us to see how the outcome is correlated to multiple predictor variables.\n",
    "\n",
    "2. In a simple linear regression, we can either use a continuous variable for a numerical predictor or an indicator variable for a categorical predictor. Indicator variables in a linear regression: $Y=\\beta_0+\\beta_11(indicator)$ allow us to capture group-specific differences like average effects for different groups. For example in a clincal study, 1=received treatment and 0=didn't get treatment. A continuous variable only uses numerical data which allows us to see trends and is denoted as $Y=\\beta_0+\\beta_1x$ \n",
    "\n",
    "3. When an indicator and continuous variable is used for multiple linear regression, it uses both in it's equation since we can have multiple predictor variables: $Y=\\beta_0+\\beta_11(indicator)+\\beta_2x_2$. In this case, the model can capture both the difference in groups (indicator) and the trend based on the continuous variable, which gives more insight into both a categorical and numerical perspective.\n",
    "\n",
    "4. The difference between having an indicator and continuous variable in a multiple linear regression vs. simple linear regression is that simple regression can only use either an indicator or a continuous variable, while multiple regression can do both and show you both perspectives: $Y=\\beta_0+\\beta_11(indicator$ x $X_2)$.\n",
    "\n",
    "5. Using only indicator variables derived from a non-binary categorical variable for a multiple linear regression captures the differences between multiple categories in the outcome variable without assuming order among them. A non-binary categorical variable is when you have more than 2 levels like colours or foods, unlike yes/no or true/false. If you have a categorical variable *C* with *k* categories, you'll create *k-1* indicator variables to avoid perfect multicollinearity, with each indicator representing one category (taking value 1 if observation is in that category and 0 otherwise). The omitted category serves as the reference group. The linear form is: $Y=\\beta_0+\\beta_1X_1+\\beta_2X_2+...+\\beta_{k-1}X_{k-1}$. The $X's$ are the indicators representing *k-1* categories. Unlike continuous variables, indicators don't have a slope so there's no trend but rather, represents categorical differences in the outcome, meaning each 'slope' coefficient just captures the mean difference in $Y$ for that category compared to the reference category. How this works is supposed we have 4 non-binary categorical variables that describe education level with categories \"highschool\", \"undergrad\", \"masters\", \"phd\". Then we have the equation $Y=\\beta_0+\\beta_1X_1+\\beta_2X_2+\\beta_3X_3$ where:\n",
    "    - $X_1$ for 'undergrad' (1 if undergrad, 0 otherwise)\n",
    "    - $X_2$ for 'masters' (1 if masters, 0 otherwise)\n",
    "    - $X_3$ for 'phd' (1 if phd, 0 otherwise)\n",
    "    \n",
    "    Suppose 'highschool' was the reference group, then $\\beta_0$ would be the mean $Y$ for individuals with 'highschool', $\\beta_1$ is the difference in mean $Y$ between 'undergrad' and 'highschool', $\\beta_2$ is the difference in mean $Y$ between 'masters' and 'highschool', and $\\beta_3$ is the difference in mean $Y$ between 'phd' and 'masters'. Basically, a multiple linear regression model based only on indicator variables from a non-binary categorical variable models category-specific mean shifts in the outcome variable. Each coefficient represents a comparison to the reference category, making the model interpretable but limited to categorical distinctions only. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc155a4",
   "metadata": {},
   "source": [
    "### 2. Explain in your own words (but working with a ChatBot if needed) what the specific (outcome and predictor) variables are for the scenario below; whether or not any meaningful interactions might need to be taken into account when predicting the outcome; and provide the linear forms with and without the potential interactions that might need to be considered<br>\n",
    "\n",
    "> Imagine a company that sells sports equipment. The company runs advertising campaigns on TV and online platforms. The effectiveness of the TV ad might depend on the amount spent on online advertising and vice versa, leading to an interaction effect between the two advertising mediums.    \n",
    "\n",
    "1. Explain how to use these two formulas to make **predictions** of the **outcome**, and give a high level explaination in general terms of the difference between **predictions** from the models with and without the **interaction** \n",
    "\n",
    "2. Explain how to update and use the implied two formulas to make predictions of the outcome if, rather than considering two continuous predictor variables, we instead suppose the advertisement budgets are simply categorized as either \"high\" or \"low\" (binary variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0323518",
   "metadata": {},
   "source": [
    "$\\beta_1 (predictor):$ amount spent on TV ads \n",
    "\n",
    "$\\beta_2 (predictor):$ amount spent on online ads\n",
    "\n",
    "$\\beta_3 (interaction):$ effectiveness of TV ads depend on amount spent on online ads &  effectiveness of online ads depend on amount spent on TV ads\n",
    "\n",
    "$Y (outcome):$ effectiveness of ads\n",
    "\n",
    "1. The first formula would be $Y=\\beta_0+\\beta_1X_1+\\beta_2X_2$ where we see how the amount spent on TV ads and the amount spent on online ads correlate to the effectiveness of the ads. In this regression, we assume the two predictors to be independent of each other with the model showing the main effects of the predictors but not their combined influence. When you have an interaction term which follows the following form: $Y=\\beta_0+\\beta_1X_1+\\beta_2X_2+\\beta_3(X_1$ x $X_2)$, you are allowing the possibility that one predictor may influence the other, so they may be dependent on each other. Without the interaction variable, the model assumes that the effect of one variable is the same regardless of the other (independent), but with an interaction term, the effect of one predictor is conditional on the other predictor. This means the relationship between the predictors and the outcome can change depending on the values of both predictors. If $\\beta_3$ was positive, then it suggests that the combination of TV and online ads is more effective than the sum of their individual effects but if it's negative, the two ads might be diminishing the overall effect.\n",
    "\n",
    "2. If the predictors are of either 'high' or 'low', we would use indicator variables with 1=high and 0=low. Our form $Y=\\beta_0+\\beta_1X_1+\\beta_2X_2+\\beta_3(X_1$ x $X_2)$ will now have $\\beta_0$ as the intercept (the outcome when $X_1$ and $X_2$ are 0 aka budgets are low), $\\beta_1$ is the effect of 'high' TV ad budget (when $X_1=1$), $\\beta_2$ is the effect of 'high' online ad budget (when $X_2=1$), and $\\beta_3$ is the interaction between TV and online ad budgets (how combination of 'high' budgets affects outcome). Depending on if the budget is high or low, we sub in 1 or 0 to $X_1$ and $X_2$ into the equation. If both budget low: $Y=\\beta_0+\\beta_1(0)+\\beta_2(0)+\\beta_3(0$ x $0)$, TV budget high but online budget low: $Y=\\beta_0+\\beta_1(1)+\\beta_2(0)+\\beta_3(1$ x $0)$, TV budget high but online budget low: $Y=\\beta_0+\\beta_1(0)+\\beta_2(1)+\\beta_3(0$ x $1)$, both budgets high: $Y=\\beta_0+\\beta_1(1)+\\beta_2(1)+\\beta_3(1$ x $1)$. So this simplifies to Low budgets = $\\beta_0$, High TV and low online: = $\\beta_0+\\beta_1$, Low TV and high online = $\\beta_0+\\beta_2$, High budgets = $\\beta_0+\\beta_1+\\beta_2+\\beta_3$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f097af67",
   "metadata": {},
   "source": [
    "### 3. Use *smf* to fit *multiple linear regression* models to the course project dataset from the canadian social connection survey<br>\n",
    "\n",
    "> **EDIT: No, you probably actually care about CATEGORICAL or BINARY outcomes rather than CONTINUOUS outcomes... so you'll probably not actually want to do _multiple linear regression_ and instead do _logistic regression_ or _multi-class classification_. Okay, I'll INSTEAD guide you through doing _logistic regression_.**\n",
    "\n",
    "1. ~~for an **additive** specification for the **linear form** based on any combination of a couple **continuous**, **binary**, and/or **categorical variables** and a **CONTINUOUS OUTCOME varaible**~~ \n",
    "    1. This would have been easy to do following the instructions [here](https://www.statsmodels.org/dev/example_formulas.html). A good alternative analagous presentation for logistic regression I just found seems to be this one from a guy named [Andrew](https://www.andrewvillazon.com/logistic-regression-python-statsmodels/). He walks you through the `logit` alternative to `OLS` given [here](https://www.statsmodels.org/dev/api.html#discrete-and-count-models).\n",
    "    2. Logistic is for a **binary outcome** so go see this [piazza post](https://piazza.com/class/m0584bs9t4thi/post/346_f1) describing how you can turn any **non-binary categorical variable** into a **binary variable**. \n",
    "    3. Then instead do this problem like this: **catogorical outcome** turned into a **binary outcome** for **logistic regression** and then use any **additive** combination of a couple of **continuous**, **binary**, and/or **categorical variables** as **predictor variables**. \n",
    "\n",
    "\n",
    "```python\n",
    "# Here's an example of how you can do this\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/KeithGalli/pandas/master/pokemon_data.csv\"\n",
    "pokeaman = pd.read_csv(url).fillna('None')\n",
    "\n",
    "pokeaman['str8fyre'] = (pokeaman['Type 1']=='Fire').astype(int)\n",
    "linear_model_specification_formula = \\\n",
    "'str8fyre ~ Attack*Legendary + Defense*I(Q(\"Type 2\")==\"None\") + C(Generation)'\n",
    "log_reg_fit = smf.logit(linear_model_specification_formula, data=pokeaman).fit()\n",
    "log_reg_fit.summary()\n",
    "```\n",
    "\n",
    "\n",
    "2. ~~for a **synertistic interaction** specification for the **linear form** based on any combination of a couple **continuous**, **binary**, and/or **categorical variables**~~\n",
    "    1. But go ahead and AGAIN do this for **logistic regression** like above.\n",
    "    2. Things are going to be A LOT simpler if you restrict yourself to **continuous** and/or **binary predictor variables**.  But of course you could *use the same trick again* to treat any **categorical variable** as just a **binary variable** (in the manner of [that piazza post](https://piazza.com/class/m0584bs9t4thi/post/346_f1).\n",
    "    \n",
    "\n",
    "3. and **interpretively explain** your **linear forms** and how to use them to make **predictions**\n",
    "    1. Look, intereting **logistic regression** *IS NOT* as simple as interpreting **multivariate linear regression**. This is because it requires you to understand so-called **log odds** and that's a bit tricky. \n",
    "    2. So, INSTEAD, **just intepret you logistic regression models** *AS IF* they were **multivariate linear regression model predictions**, okay?\n",
    "\n",
    "\n",
    "4. and interpret the statistical evidence associated with the **predictor variables** for each of your model specifications \n",
    "    1. **Yeah, you're going to be able to do this based on the `.fit().summary()` table _just like with multiple linear regression_**... now you might be starting to see how AWESOME all of this stuff we're doing is going to be able to get...\n",
    "\n",
    "\n",
    "5. and finally use `plotly` to visualize the data with corresponding \"best fit lines\" for a model with **continuous** plus **binary indicator** specification under both (a) **additive** and (b) **synergistic** specifications of the **linear form** (on separate figures), commenting on the apparent necessity (or lack thereof) of the **interaction** term for the data in question\n",
    "    1. Aw, shit, you DEF not going to be able to do this if you're doing **logistic regression** because of that **log odds** thing I mentioned... hmm...\n",
    "    2. OKAY! Just *pretend* it's **multivariate linear regression** (even if you're doing **logistic regression**) and *pretend* your **fitted coefficients** belong to a **continuous** and a **binary predictor variable**; then, draw the lines as requested, and simulate **random noise** for the values of your **predictor data** and plot your lines along with that data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e14efb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- What I'm doing in the code below is I'm reading in the survey data and filling all NA values with None so there's not much errors in coding. \n",
    "\n",
    "- Next, because there aren't many continuous data in the survey, I convert the outcome variable that I want (which is categorical) into a binary variable where I only want to evaluate when the surveyer answered 'Often' for feeling left out. I'm also saving this as a new column (variable) called 'feeling_left_out' just so that it's shorter\n",
    "\n",
    "- I then create my linear form to prepare it for regression. Again, I'm using 'feeling_left_out' as my outcome variable (where it's a binary variable only accounting for when people 'Often' felt left out). I have 2 predictor variables, 1 is an interaction variable between self-rated mental health and how aware people are of how lonely they are, and 2 is a categorical variable grouping the data into how many close friends the person has\n",
    "\n",
    "Interaction term explaination:\n",
    "- Again, because there aren't really continuous variables, I'm turning categorical variables into indicator variables where I only care about certain results. So *I(Q(\"WELLNESS_self_rated_mental_health\")==\"Very good\")* means I want to evaluate the person's self rated mental health when it is 'Very Good'. The Q() just means that I'm accounting for column names that may contain spaces or special characters. The I() means I want this to be an indicator variable (since I want numerical data because it makes it easier) so when the model goes over the data and finds a data point where someone rated 'Very Good', it will count that as 1 until it goes over all of the data and finds out how many people voted 'Very Good' for that question.\n",
    "- The same thing goes for *I(Q(\"LONELY_others_aware\")==\"Definitely No\"* where I'm evaluating when people voted 'Definitely No' for if other people are aware of how lonely they are. \n",
    "- I use * between them because I want to see the interaction between a very good self-rated mental health but a lowly rated lonely awareness, which may indicate that the person keeps themselves closed off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "654bcdb2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_307/722802404.py:4: DtypeWarning: Columns (408,1001,1002,1006,1007,1008,1080,1113,1115,1116,1117,1118,1119,1120,1121,1124,1125,1126,1127,1128,1213,1214,1215,1216,1217,1218,1342,1343,1344,1345,1346,1347,1348,1349,1390,1391,1393,1463,1549,1552,1555,1558,1561) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  csdata = pd.read_csv('CSCS_data_anon.csv').fillna('None')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.208562\n",
      "         Iterations: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_307/722802404.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  csdata['feeling_left_out'] = (csdata['LONELY_ucla_loneliness_scale_left_out']=='Often').astype(int)\n",
      "/opt/conda/lib/python3.11/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>   <td>feeling_left_out</td> <th>  No. Observations:  </th>   <td> 11431</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td> 11423</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     7</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Thu, 14 Nov 2024</td> <th>  Pseudo R-squ.:     </th>   <td>0.1030</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>01:25:16</td>     <th>  Log-Likelihood:    </th>  <td> -2384.1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>False</td>      <th>  LL-Null:           </th>  <td> -2657.7</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>5.374e-114</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                                                             <td></td>                                                                <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                                                                                                               <td>   -2.2472</td> <td>    0.072</td> <td>  -31.013</td> <td> 0.000</td> <td>   -2.389</td> <td>   -2.105</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(Q(\"WELLNESS_self_rated_mental_health\") == \"Very good\")[T.True]</th>                                                        <td>   -0.5051</td> <td>    0.120</td> <td>   -4.199</td> <td> 0.000</td> <td>   -0.741</td> <td>   -0.269</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(Q(\"LONELY_others_aware\") == \"Definitely No\")[T.True]</th>                                                                  <td>    1.8588</td> <td>    0.113</td> <td>   16.377</td> <td> 0.000</td> <td>    1.636</td> <td>    2.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(CONNECTION_social_num_close_friends_grouped)[T.3–4]</th>                                                                   <td>   -0.1467</td> <td>    0.094</td> <td>   -1.556</td> <td> 0.120</td> <td>   -0.331</td> <td>    0.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(CONNECTION_social_num_close_friends_grouped)[T.5 or more]</th>                                                             <td>   -1.1417</td> <td>    0.127</td> <td>   -8.986</td> <td> 0.000</td> <td>   -1.391</td> <td>   -0.893</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(CONNECTION_social_num_close_friends_grouped)[T.None]</th>                                                                  <td>   -1.5166</td> <td>    0.130</td> <td>  -11.650</td> <td> 0.000</td> <td>   -1.772</td> <td>   -1.261</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(CONNECTION_social_num_close_friends_grouped)[T.Presented but no response]</th>                                             <td>  -28.1250</td> <td> 7.18e+05</td> <td>-3.91e-05</td> <td> 1.000</td> <td>-1.41e+06</td> <td> 1.41e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(Q(\"WELLNESS_self_rated_mental_health\") == \"Very good\")[T.True]:I(Q(\"LONELY_others_aware\") == \"Definitely No\")[T.True]</th> <td>   -0.6454</td> <td>    0.300</td> <td>   -2.153</td> <td> 0.031</td> <td>   -1.233</td> <td>   -0.058</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                                                                                                                & feeling\\_left\\_out & \\textbf{  No. Observations:  } &    11431    \\\\\n",
       "\\textbf{Model:}                                                                                                                        &       Logit        & \\textbf{  Df Residuals:      } &    11423    \\\\\n",
       "\\textbf{Method:}                                                                                                                       &        MLE         & \\textbf{  Df Model:          } &        7    \\\\\n",
       "\\textbf{Date:}                                                                                                                         &  Thu, 14 Nov 2024  & \\textbf{  Pseudo R-squ.:     } &   0.1030    \\\\\n",
       "\\textbf{Time:}                                                                                                                         &      01:25:16      & \\textbf{  Log-Likelihood:    } &   -2384.1   \\\\\n",
       "\\textbf{converged:}                                                                                                                    &       False        & \\textbf{  LL-Null:           } &   -2657.7   \\\\\n",
       "\\textbf{Covariance Type:}                                                                                                              &     nonrobust      & \\textbf{  LLR p-value:       } & 5.374e-114  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                                                                                                                       & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                                                                                                                     &      -2.2472  &        0.072     &   -31.013  &         0.000        &       -2.389    &       -2.105     \\\\\n",
       "\\textbf{I(Q(\"WELLNESS\\_self\\_rated\\_mental\\_health\") == \"Very good\")[T.True]}                                                          &      -0.5051  &        0.120     &    -4.199  &         0.000        &       -0.741    &       -0.269     \\\\\n",
       "\\textbf{I(Q(\"LONELY\\_others\\_aware\") == \"Definitely No\")[T.True]}                                                                      &       1.8588  &        0.113     &    16.377  &         0.000        &        1.636    &        2.081     \\\\\n",
       "\\textbf{C(CONNECTION\\_social\\_num\\_close\\_friends\\_grouped)[T.3–4]}                                                                    &      -0.1467  &        0.094     &    -1.556  &         0.120        &       -0.331    &        0.038     \\\\\n",
       "\\textbf{C(CONNECTION\\_social\\_num\\_close\\_friends\\_grouped)[T.5 or more]}                                                              &      -1.1417  &        0.127     &    -8.986  &         0.000        &       -1.391    &       -0.893     \\\\\n",
       "\\textbf{C(CONNECTION\\_social\\_num\\_close\\_friends\\_grouped)[T.None]}                                                                   &      -1.5166  &        0.130     &   -11.650  &         0.000        &       -1.772    &       -1.261     \\\\\n",
       "\\textbf{C(CONNECTION\\_social\\_num\\_close\\_friends\\_grouped)[T.Presented but no response]}                                              &     -28.1250  &     7.18e+05     & -3.91e-05  &         1.000        &    -1.41e+06    &     1.41e+06     \\\\\n",
       "\\textbf{I(Q(\"WELLNESS\\_self\\_rated\\_mental\\_health\") == \"Very good\")[T.True]:I(Q(\"LONELY\\_others\\_aware\") == \"Definitely No\")[T.True]} &      -0.6454  &        0.300     &    -2.153  &         0.031        &       -1.233    &       -0.058     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{Logit Regression Results}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:       feeling_left_out   No. Observations:                11431\n",
       "Model:                          Logit   Df Residuals:                    11423\n",
       "Method:                           MLE   Df Model:                            7\n",
       "Date:                Thu, 14 Nov 2024   Pseudo R-squ.:                  0.1030\n",
       "Time:                        01:25:16   Log-Likelihood:                -2384.1\n",
       "converged:                      False   LL-Null:                       -2657.7\n",
       "Covariance Type:            nonrobust   LLR p-value:                5.374e-114\n",
       "===========================================================================================================================================================================================\n",
       "                                                                                                                              coef    std err          z      P>|z|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "Intercept                                                                                                                  -2.2472      0.072    -31.013      0.000      -2.389      -2.105\n",
       "I(Q(\"WELLNESS_self_rated_mental_health\") == \"Very good\")[T.True]                                                           -0.5051      0.120     -4.199      0.000      -0.741      -0.269\n",
       "I(Q(\"LONELY_others_aware\") == \"Definitely No\")[T.True]                                                                      1.8588      0.113     16.377      0.000       1.636       2.081\n",
       "C(CONNECTION_social_num_close_friends_grouped)[T.3–4]                                                                      -0.1467      0.094     -1.556      0.120      -0.331       0.038\n",
       "C(CONNECTION_social_num_close_friends_grouped)[T.5 or more]                                                                -1.1417      0.127     -8.986      0.000      -1.391      -0.893\n",
       "C(CONNECTION_social_num_close_friends_grouped)[T.None]                                                                     -1.5166      0.130    -11.650      0.000      -1.772      -1.261\n",
       "C(CONNECTION_social_num_close_friends_grouped)[T.Presented but no response]                                               -28.1250   7.18e+05  -3.91e-05      1.000   -1.41e+06    1.41e+06\n",
       "I(Q(\"WELLNESS_self_rated_mental_health\") == \"Very good\")[T.True]:I(Q(\"LONELY_others_aware\") == \"Definitely No\")[T.True]    -0.6454      0.300     -2.153      0.031      -1.233      -0.058\n",
       "===========================================================================================================================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "csdata = pd.read_csv('CSCS_data_anon.csv').fillna('None')\n",
    "\n",
    "csdata['feeling_left_out'] = (csdata['LONELY_ucla_loneliness_scale_left_out']=='Often').astype(int)\n",
    "linear_model_specification_formula = \\\n",
    "'feeling_left_out ~ I(Q(\"WELLNESS_self_rated_mental_health\")==\"Very good\")*I(Q(\"LONELY_others_aware\")==\"Definitely No\") + C(CONNECTION_social_num_close_friends_grouped)'\n",
    "log_reg_fit = smf.logit(linear_model_specification_formula, data=csdata).fit()\n",
    "log_reg_fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f502ff6",
   "metadata": {},
   "source": [
    "Based on the p-values for many of the predictor variables, we can reject the null hypothesis that a very good mental health, how aware other people are of how lonely they are, having 5 or more friends, or that having no friends has no effect on how left out the person feels. This means these variables have a significant effect on how left out they feel, while we fail to reject that 3-4 close friends has any effect, meaning it has no effect. For the interaction term, the coefficient is around -0.6 which indicates that as one predictor goes up, the other goes down. We can also see that it's p-value is 0.03 which by the p-value table, indicates that we have moderate evidence against the null hypothesis of no effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "678113fa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.232400\n",
      "         Iterations 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create the dependent variable for logistic regression\n",
    "csdata['feeling_left_out'] = (csdata['LONELY_ucla_loneliness_scale_left_out'] == 'Often').astype(int)\n",
    "\n",
    "# Simulate random noise for the predictor variables (continuous and binary)\n",
    "np.random.seed(42)\n",
    "csdata['WELLNESS_self_rated_mental_health_noise'] = np.random.choice(\n",
    "    ['Poor', 'Fair', 'Good', 'Very good', 'Excellent'], size=csdata.shape[0], p=[0.1, 0.2, 0.3, 0.2, 0.2])\n",
    "csdata['LONELY_others_aware_noise'] = np.random.choice(['Yes', 'No'], size=csdata.shape[0], p=[0.3, 0.7])\n",
    "\n",
    "# Recode binary variables as numerical values for simplicity in plotting\n",
    "csdata['WELLNESS_self_rated_mental_health_binary'] = csdata['WELLNESS_self_rated_mental_health_noise'].map(\n",
    "    {'Poor': 0, 'Fair': 1, 'Good': 2, 'Very good': 3, 'Excellent': 4})\n",
    "\n",
    "csdata['LONELY_others_aware_binary'] = csdata['LONELY_others_aware_noise'].map({'No': 0, 'Yes': 1})\n",
    "\n",
    "# Model Specification (simulating linear regression with binary and continuous predictors)\n",
    "linear_model_specification_formula = 'feeling_left_out ~ WELLNESS_self_rated_mental_health_binary * LONELY_others_aware_binary'\n",
    "\n",
    "# Fit the logistic regression model\n",
    "log_reg_fit = smf.logit(linear_model_specification_formula, data=csdata).fit()\n",
    "\n",
    "# Generate predicted probabilities\n",
    "csdata['predicted_probs'] = log_reg_fit.predict(csdata)\n",
    "\n",
    "# Create scatter plot for the noisy data with lines representing predicted probabilities\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add scatter points with random noise for the predictors and observed outcome\n",
    "fig.add_trace(go.Scatter(x=csdata['WELLNESS_self_rated_mental_health_binary'], \n",
    "                         y=csdata['feeling_left_out'], \n",
    "                         mode='markers', \n",
    "                         name='Observed Data',\n",
    "                         marker=dict(color=csdata['feeling_left_out'], colorscale='Viridis', size=6, opacity=0.6)))\n",
    "\n",
    "# Plot lines for the fitted coefficients - simulate continuous line across the range of binary + continuous predictors\n",
    "for wellness_value in csdata['WELLNESS_self_rated_mental_health_binary'].unique():\n",
    "    for lonely_value in csdata['LONELY_others_aware_binary'].unique():\n",
    "        subset_data = csdata[(csdata['WELLNESS_self_rated_mental_health_binary'] == wellness_value) & \n",
    "                             (csdata['LONELY_others_aware_binary'] == lonely_value)]\n",
    "        if len(subset_data) > 0:\n",
    "            # Fit a linear regression line as if it's continuous for each condition\n",
    "            x_vals = np.linspace(subset_data['WELLNESS_self_rated_mental_health_binary'].min(), \n",
    "                                 subset_data['WELLNESS_self_rated_mental_health_binary'].max(), 100)\n",
    "            y_vals = log_reg_fit.predict(pd.DataFrame({\n",
    "                'WELLNESS_self_rated_mental_health_binary': x_vals, \n",
    "                'LONELY_others_aware_binary': lonely_value}))\n",
    "            \n",
    "            # Add the best fit line for each combination of wellness and loneliness\n",
    "            fig.add_trace(go.Scatter(x=x_vals, y=y_vals, mode='lines', \n",
    "                                     name=f'Fit Line - Wellness: {wellness_value}, Loneliness: {lonely_value}'))\n",
    "\n",
    "# Update layout to make the plot clearer\n",
    "fig.update_layout(\n",
    "    title=\"Simulated Multivariate Linear Regression with Logistic Data\",\n",
    "    xaxis_title=\"Wellness Rating (Binary)\",\n",
    "    yaxis_title=\"Feeling Left Out (1=Yes, 0=No)\",\n",
    "    legend_title=\"Wellness and Loneliness Levels\",\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "fig.show(renderer=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6ac555",
   "metadata": {},
   "source": [
    "### 4. Explain the apparent contradiction between the factual statements regarding the fit below that \"the model only explains 17.6% of the variability in the data\" while at the same time \"many of the *coefficients* are larger than 10 while having *strong* or *very strong evidence against* the *null hypothesis* of 'no effect'\"<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee4ccce6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>Name</th>\n",
       "      <th>Type 1</th>\n",
       "      <th>Type 2</th>\n",
       "      <th>HP</th>\n",
       "      <th>Attack</th>\n",
       "      <th>Defense</th>\n",
       "      <th>Sp. Atk</th>\n",
       "      <th>Sp. Def</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Generation</th>\n",
       "      <th>Legendary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Bulbasaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>45</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Ivysaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>60</td>\n",
       "      <td>62</td>\n",
       "      <td>63</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Venusaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>80</td>\n",
       "      <td>82</td>\n",
       "      <td>83</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>VenusaurMega Venusaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>80</td>\n",
       "      <td>100</td>\n",
       "      <td>123</td>\n",
       "      <td>122</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Charmander</td>\n",
       "      <td>Fire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39</td>\n",
       "      <td>52</td>\n",
       "      <td>43</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>719</td>\n",
       "      <td>Diancie</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Fairy</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>150</td>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>719</td>\n",
       "      <td>DiancieMega Diancie</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Fairy</td>\n",
       "      <td>50</td>\n",
       "      <td>160</td>\n",
       "      <td>110</td>\n",
       "      <td>160</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>720</td>\n",
       "      <td>HoopaHoopa Confined</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>Ghost</td>\n",
       "      <td>80</td>\n",
       "      <td>110</td>\n",
       "      <td>60</td>\n",
       "      <td>150</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>720</td>\n",
       "      <td>HoopaHoopa Unbound</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>Dark</td>\n",
       "      <td>80</td>\n",
       "      <td>160</td>\n",
       "      <td>60</td>\n",
       "      <td>170</td>\n",
       "      <td>130</td>\n",
       "      <td>80</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>721</td>\n",
       "      <td>Volcanion</td>\n",
       "      <td>Fire</td>\n",
       "      <td>Water</td>\n",
       "      <td>80</td>\n",
       "      <td>110</td>\n",
       "      <td>120</td>\n",
       "      <td>130</td>\n",
       "      <td>90</td>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       #                   Name   Type 1  Type 2  HP  Attack  Defense  \\\n",
       "0      1              Bulbasaur    Grass  Poison  45      49       49   \n",
       "1      2                Ivysaur    Grass  Poison  60      62       63   \n",
       "2      3               Venusaur    Grass  Poison  80      82       83   \n",
       "3      3  VenusaurMega Venusaur    Grass  Poison  80     100      123   \n",
       "4      4             Charmander     Fire     NaN  39      52       43   \n",
       "..   ...                    ...      ...     ...  ..     ...      ...   \n",
       "795  719                Diancie     Rock   Fairy  50     100      150   \n",
       "796  719    DiancieMega Diancie     Rock   Fairy  50     160      110   \n",
       "797  720    HoopaHoopa Confined  Psychic   Ghost  80     110       60   \n",
       "798  720     HoopaHoopa Unbound  Psychic    Dark  80     160       60   \n",
       "799  721              Volcanion     Fire   Water  80     110      120   \n",
       "\n",
       "     Sp. Atk  Sp. Def  Speed  Generation  Legendary  \n",
       "0         65       65     45           1      False  \n",
       "1         80       80     60           1      False  \n",
       "2        100      100     80           1      False  \n",
       "3        122      120     80           1      False  \n",
       "4         60       50     65           1      False  \n",
       "..       ...      ...    ...         ...        ...  \n",
       "795      100      150     50           6       True  \n",
       "796      160      110    110           6       True  \n",
       "797      150      130     70           6       True  \n",
       "798      170      130     80           6       True  \n",
       "799      130       90     70           6       True  \n",
       "\n",
       "[800 rows x 12 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/KeithGalli/pandas/master/pokemon_data.csv\"\n",
    "# fail https://github.com/KeithGalli/pandas/blob/master/pokemon_data.csv\n",
    "pokeaman = pd.read_csv(url) \n",
    "pokeaman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d4a80b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   15.27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 13 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>3.50e-27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:53:04</td>     <th>  Log-Likelihood:    </th> <td> -3649.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   800</td>      <th>  AIC:               </th> <td>   7323.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   788</td>      <th>  BIC:               </th> <td>   7379.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    11</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                 <td></td>                    <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                       <td>   26.8971</td> <td>    5.246</td> <td>    5.127</td> <td> 0.000</td> <td>   16.599</td> <td>   37.195</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.2]</th>              <td>   20.0449</td> <td>    7.821</td> <td>    2.563</td> <td> 0.011</td> <td>    4.692</td> <td>   35.398</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.3]</th>              <td>   21.3662</td> <td>    6.998</td> <td>    3.053</td> <td> 0.002</td> <td>    7.629</td> <td>   35.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.4]</th>              <td>   31.9575</td> <td>    8.235</td> <td>    3.881</td> <td> 0.000</td> <td>   15.793</td> <td>   48.122</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.5]</th>              <td>    9.4926</td> <td>    7.883</td> <td>    1.204</td> <td> 0.229</td> <td>   -5.982</td> <td>   24.968</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.6]</th>              <td>   22.2693</td> <td>    8.709</td> <td>    2.557</td> <td> 0.011</td> <td>    5.173</td> <td>   39.366</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\")</th>                    <td>    0.5634</td> <td>    0.071</td> <td>    7.906</td> <td> 0.000</td> <td>    0.423</td> <td>    0.703</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.2]</th> <td>   -0.2350</td> <td>    0.101</td> <td>   -2.316</td> <td> 0.021</td> <td>   -0.434</td> <td>   -0.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.3]</th> <td>   -0.3067</td> <td>    0.093</td> <td>   -3.300</td> <td> 0.001</td> <td>   -0.489</td> <td>   -0.124</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.4]</th> <td>   -0.3790</td> <td>    0.105</td> <td>   -3.600</td> <td> 0.000</td> <td>   -0.586</td> <td>   -0.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.5]</th> <td>   -0.0484</td> <td>    0.108</td> <td>   -0.447</td> <td> 0.655</td> <td>   -0.261</td> <td>    0.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.6]</th> <td>   -0.3083</td> <td>    0.112</td> <td>   -2.756</td> <td> 0.006</td> <td>   -0.528</td> <td>   -0.089</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>337.229</td> <th>  Durbin-Watson:     </th> <td>   1.505</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2871.522</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.684</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>11.649</td>  <th>  Cond. No.          </th> <td>1.40e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.4e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                  &        HP        & \\textbf{  R-squared:         } &     0.176   \\\\\n",
       "\\textbf{Model:}                          &       OLS        & \\textbf{  Adj. R-squared:    } &     0.164   \\\\\n",
       "\\textbf{Method:}                         &  Least Squares   & \\textbf{  F-statistic:       } &     15.27   \\\\\n",
       "\\textbf{Date:}                           & Wed, 13 Nov 2024 & \\textbf{  Prob (F-statistic):} &  3.50e-27   \\\\\n",
       "\\textbf{Time:}                           &     21:53:04     & \\textbf{  Log-Likelihood:    } &   -3649.4   \\\\\n",
       "\\textbf{No. Observations:}               &         800      & \\textbf{  AIC:               } &     7323.   \\\\\n",
       "\\textbf{Df Residuals:}                   &         788      & \\textbf{  BIC:               } &     7379.   \\\\\n",
       "\\textbf{Df Model:}                       &          11      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}                &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                         & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                       &      26.8971  &        5.246     &     5.127  &         0.000        &       16.599    &       37.195     \\\\\n",
       "\\textbf{C(Generation)[T.2]}              &      20.0449  &        7.821     &     2.563  &         0.011        &        4.692    &       35.398     \\\\\n",
       "\\textbf{C(Generation)[T.3]}              &      21.3662  &        6.998     &     3.053  &         0.002        &        7.629    &       35.103     \\\\\n",
       "\\textbf{C(Generation)[T.4]}              &      31.9575  &        8.235     &     3.881  &         0.000        &       15.793    &       48.122     \\\\\n",
       "\\textbf{C(Generation)[T.5]}              &       9.4926  &        7.883     &     1.204  &         0.229        &       -5.982    &       24.968     \\\\\n",
       "\\textbf{C(Generation)[T.6]}              &      22.2693  &        8.709     &     2.557  &         0.011        &        5.173    &       39.366     \\\\\n",
       "\\textbf{Q(\"Sp. Def\")}                    &       0.5634  &        0.071     &     7.906  &         0.000        &        0.423    &        0.703     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.2]} &      -0.2350  &        0.101     &    -2.316  &         0.021        &       -0.434    &       -0.036     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.3]} &      -0.3067  &        0.093     &    -3.300  &         0.001        &       -0.489    &       -0.124     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.4]} &      -0.3790  &        0.105     &    -3.600  &         0.000        &       -0.586    &       -0.172     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.5]} &      -0.0484  &        0.108     &    -0.447  &         0.655        &       -0.261    &        0.164     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.6]} &      -0.3083  &        0.112     &    -2.756  &         0.006        &       -0.528    &       -0.089     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 337.229 & \\textbf{  Durbin-Watson:     } &    1.505  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 2871.522  \\\\\n",
       "\\textbf{Skew:}          &   1.684 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  11.649 & \\textbf{  Cond. No.          } & 1.40e+03  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 1.4e+03. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.176\n",
       "Model:                            OLS   Adj. R-squared:                  0.164\n",
       "Method:                 Least Squares   F-statistic:                     15.27\n",
       "Date:                Wed, 13 Nov 2024   Prob (F-statistic):           3.50e-27\n",
       "Time:                        21:53:04   Log-Likelihood:                -3649.4\n",
       "No. Observations:                 800   AIC:                             7323.\n",
       "Df Residuals:                     788   BIC:                             7379.\n",
       "Df Model:                          11                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===================================================================================================\n",
       "                                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------------------------\n",
       "Intercept                          26.8971      5.246      5.127      0.000      16.599      37.195\n",
       "C(Generation)[T.2]                 20.0449      7.821      2.563      0.011       4.692      35.398\n",
       "C(Generation)[T.3]                 21.3662      6.998      3.053      0.002       7.629      35.103\n",
       "C(Generation)[T.4]                 31.9575      8.235      3.881      0.000      15.793      48.122\n",
       "C(Generation)[T.5]                  9.4926      7.883      1.204      0.229      -5.982      24.968\n",
       "C(Generation)[T.6]                 22.2693      8.709      2.557      0.011       5.173      39.366\n",
       "Q(\"Sp. Def\")                        0.5634      0.071      7.906      0.000       0.423       0.703\n",
       "Q(\"Sp. Def\"):C(Generation)[T.2]    -0.2350      0.101     -2.316      0.021      -0.434      -0.036\n",
       "Q(\"Sp. Def\"):C(Generation)[T.3]    -0.3067      0.093     -3.300      0.001      -0.489      -0.124\n",
       "Q(\"Sp. Def\"):C(Generation)[T.4]    -0.3790      0.105     -3.600      0.000      -0.586      -0.172\n",
       "Q(\"Sp. Def\"):C(Generation)[T.5]    -0.0484      0.108     -0.447      0.655      -0.261       0.164\n",
       "Q(\"Sp. Def\"):C(Generation)[T.6]    -0.3083      0.112     -2.756      0.006      -0.528      -0.089\n",
       "==============================================================================\n",
       "Omnibus:                      337.229   Durbin-Watson:                   1.505\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2871.522\n",
       "Skew:                           1.684   Prob(JB):                         0.00\n",
       "Kurtosis:                      11.649   Cond. No.                     1.40e+03\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.4e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "model1_spec = smf.ols(formula='HP ~ Q(\"Sp. Def\") + C(Generation)', data=pokeaman)\n",
    "model2_spec = smf.ols(formula='HP ~ Q(\"Sp. Def\") + C(Generation) + Q(\"Sp. Def\"):C(Generation)', data=pokeaman)\n",
    "model2_spec = smf.ols(formula='HP ~ Q(\"Sp. Def\") * C(Generation)', data=pokeaman)\n",
    "\n",
    "model2_fit = model2_spec.fit()\n",
    "model2_fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9237d18",
   "metadata": {},
   "source": [
    "A low $R^2$ value but significant p-values may indicate that the predictor variables have a significant relationship with the outcome variable, there may be stronger variables that account for more of the variance in the outcome so the current predictors have less of an effect. In a sense, 'Generation' and 'Sp. Def' may not capture most of the factors that influence 'HP'. Alternatively, too large of a sample size, non-linear but assuming linear relationship, or measurement error in the variables also result in a low $R^2$ but significant p-values. In this case, the note at the bottom of the second code tells us that the regression assumptions are being followed, but that there may be multicollinearity in the variables. This means that the predictor variables are correlated with each other and although they each show significant p-values with the outcome, they are redundant and provide little new information to explain the outcome. As in the code, the same/similar data is being used again which clearly shows multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51edb40a",
   "metadata": {},
   "source": [
    "### 5. Discuss the following (five cells of) code and results with a ChatBot and based on the understanding you arrive at in this conversation explain what the following (five cells of) are illustrating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4dd90ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>Name</th>\n",
       "      <th>Type 1</th>\n",
       "      <th>Type 2</th>\n",
       "      <th>HP</th>\n",
       "      <th>Attack</th>\n",
       "      <th>Defense</th>\n",
       "      <th>Sp. Atk</th>\n",
       "      <th>Sp. Def</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Generation</th>\n",
       "      <th>Legendary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>338</td>\n",
       "      <td>Solrock</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>70</td>\n",
       "      <td>95</td>\n",
       "      <td>85</td>\n",
       "      <td>55</td>\n",
       "      <td>65</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Charizard</td>\n",
       "      <td>Fire</td>\n",
       "      <td>Flying</td>\n",
       "      <td>78</td>\n",
       "      <td>84</td>\n",
       "      <td>78</td>\n",
       "      <td>109</td>\n",
       "      <td>85</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>224</td>\n",
       "      <td>Octillery</td>\n",
       "      <td>Water</td>\n",
       "      <td>None</td>\n",
       "      <td>75</td>\n",
       "      <td>105</td>\n",
       "      <td>75</td>\n",
       "      <td>105</td>\n",
       "      <td>75</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>600</td>\n",
       "      <td>Klang</td>\n",
       "      <td>Steel</td>\n",
       "      <td>None</td>\n",
       "      <td>60</td>\n",
       "      <td>80</td>\n",
       "      <td>95</td>\n",
       "      <td>70</td>\n",
       "      <td>85</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>265</td>\n",
       "      <td>Wurmple</td>\n",
       "      <td>Bug</td>\n",
       "      <td>None</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>35</td>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>471</td>\n",
       "      <td>Glaceon</td>\n",
       "      <td>Ice</td>\n",
       "      <td>None</td>\n",
       "      <td>65</td>\n",
       "      <td>60</td>\n",
       "      <td>110</td>\n",
       "      <td>130</td>\n",
       "      <td>95</td>\n",
       "      <td>65</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>225</td>\n",
       "      <td>Delibird</td>\n",
       "      <td>Ice</td>\n",
       "      <td>Flying</td>\n",
       "      <td>45</td>\n",
       "      <td>55</td>\n",
       "      <td>45</td>\n",
       "      <td>65</td>\n",
       "      <td>45</td>\n",
       "      <td>75</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>720</td>\n",
       "      <td>HoopaHoopa Confined</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>Ghost</td>\n",
       "      <td>80</td>\n",
       "      <td>110</td>\n",
       "      <td>60</td>\n",
       "      <td>150</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>109</td>\n",
       "      <td>Koffing</td>\n",
       "      <td>Poison</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>65</td>\n",
       "      <td>95</td>\n",
       "      <td>60</td>\n",
       "      <td>45</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>373</td>\n",
       "      <td>SalamenceMega Salamence</td>\n",
       "      <td>Dragon</td>\n",
       "      <td>Flying</td>\n",
       "      <td>95</td>\n",
       "      <td>145</td>\n",
       "      <td>130</td>\n",
       "      <td>120</td>\n",
       "      <td>90</td>\n",
       "      <td>120</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       #                     Name   Type 1   Type 2  HP  Attack  Defense  \\\n",
       "370  338                  Solrock     Rock  Psychic  70      95       85   \n",
       "6      6                Charizard     Fire   Flying  78      84       78   \n",
       "242  224                Octillery    Water     None  75     105       75   \n",
       "661  600                    Klang    Steel     None  60      80       95   \n",
       "288  265                  Wurmple      Bug     None  45      45       35   \n",
       "..   ...                      ...      ...      ...  ..     ...      ...   \n",
       "522  471                  Glaceon      Ice     None  65      60      110   \n",
       "243  225                 Delibird      Ice   Flying  45      55       45   \n",
       "797  720      HoopaHoopa Confined  Psychic    Ghost  80     110       60   \n",
       "117  109                  Koffing   Poison     None  40      65       95   \n",
       "409  373  SalamenceMega Salamence   Dragon   Flying  95     145      130   \n",
       "\n",
       "     Sp. Atk  Sp. Def  Speed  Generation  Legendary  \n",
       "370       55       65     70           3      False  \n",
       "6        109       85    100           1      False  \n",
       "242      105       75     45           2      False  \n",
       "661       70       85     50           5      False  \n",
       "288       20       30     20           3      False  \n",
       "..       ...      ...    ...         ...        ...  \n",
       "522      130       95     65           4      False  \n",
       "243       65       45     75           2      False  \n",
       "797      150      130     70           6       True  \n",
       "117       60       45     35           1      False  \n",
       "409      120       90    120           3      False  \n",
       "\n",
       "[400 rows x 12 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "fifty_fifty_split_size = int(pokeaman.shape[0]*0.5)\n",
    "\n",
    "# Replace \"NaN\" (in the \"Type 2\" column with \"None\")\n",
    "pokeaman.fillna('None', inplace=True)\n",
    "\n",
    "np.random.seed(130)\n",
    "pokeaman_train,pokeaman_test = \\\n",
    "  train_test_split(pokeaman, train_size=fifty_fifty_split_size)\n",
    "pokeaman_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18af1366",
   "metadata": {},
   "source": [
    "*train_test_split* is a tool used for splitting data into training and test sets. *fifty_fifty_split_size* takes the number of rows in *pokeaman* and splits it in half, so the number of rows split in half is stored in the variable for later use. We then replace all NaN values in the dataset with 'None' but since NaN values only exist in the Type 2 column, we're essentially only performing this on that column. Setting a random seed lets us randomly generate an effect but be able to reproduce that random draw of numbers. Now we use *train_test_split* and initialize it with two variables, *pokeaman_train* and *pokeaman_test* since the *train_test_split* function splits into a training and a test set. We then pass through the pokeaman dataset and only train about half of the data (row-wise). The training set is used to train the machine learning model which will learn patterns, relationships, and structures in the data. It then uses the data in the training set to make predictions, and based on how well those predictions match the actual outcomes, the model adjusts it's internal parameters to improve accuracy. The test set is used to evaluate the performance of the trained model. After being trained in the training set, it's then applied to the test set (which it has never seen before) to see how well the trained model works against unseen data (hence, why we split the data in half). Testing it allows us to see how well the model performs in a real-world scenario, which helps us identify overfitting (where model performs perfectly on training data but poorly on new data) or underfitting (where model doesn't perform well on either set). Training and testing the data ensures that the model isn't just memorizing the data, but can generalize to new, unseen situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69a8b379",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   34.40</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 13 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>1.66e-14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:53:04</td>     <th>  Log-Likelihood:    </th> <td> -1832.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   3671.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   397</td>      <th>  BIC:               </th> <td>   3683.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   42.5882</td> <td>    3.580</td> <td>   11.897</td> <td> 0.000</td> <td>   35.551</td> <td>   49.626</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack</th>    <td>    0.2472</td> <td>    0.041</td> <td>    6.051</td> <td> 0.000</td> <td>    0.167</td> <td>    0.327</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense</th>   <td>    0.1001</td> <td>    0.045</td> <td>    2.201</td> <td> 0.028</td> <td>    0.011</td> <td>    0.190</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>284.299</td> <th>  Durbin-Watson:     </th> <td>   2.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>5870.841</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.720</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>20.963</td>  <th>  Cond. No.          </th> <td>    343.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &        HP        & \\textbf{  R-squared:         } &     0.148   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.143   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     34.40   \\\\\n",
       "\\textbf{Date:}             & Wed, 13 Nov 2024 & \\textbf{  Prob (F-statistic):} &  1.66e-14   \\\\\n",
       "\\textbf{Time:}             &     21:53:04     & \\textbf{  Log-Likelihood:    } &   -1832.6   \\\\\n",
       "\\textbf{No. Observations:} &         400      & \\textbf{  AIC:               } &     3671.   \\\\\n",
       "\\textbf{Df Residuals:}     &         397      & \\textbf{  BIC:               } &     3683.   \\\\\n",
       "\\textbf{Df Model:}         &           2      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept} &      42.5882  &        3.580     &    11.897  &         0.000        &       35.551    &       49.626     \\\\\n",
       "\\textbf{Attack}    &       0.2472  &        0.041     &     6.051  &         0.000        &        0.167    &        0.327     \\\\\n",
       "\\textbf{Defense}   &       0.1001  &        0.045     &     2.201  &         0.028        &        0.011    &        0.190     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 284.299 & \\textbf{  Durbin-Watson:     } &    2.006  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 5870.841  \\\\\n",
       "\\textbf{Skew:}          &   2.720 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  20.963 & \\textbf{  Cond. No.          } &     343.  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.148\n",
       "Model:                            OLS   Adj. R-squared:                  0.143\n",
       "Method:                 Least Squares   F-statistic:                     34.40\n",
       "Date:                Wed, 13 Nov 2024   Prob (F-statistic):           1.66e-14\n",
       "Time:                        21:53:04   Log-Likelihood:                -1832.6\n",
       "No. Observations:                 400   AIC:                             3671.\n",
       "Df Residuals:                     397   BIC:                             3683.\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     42.5882      3.580     11.897      0.000      35.551      49.626\n",
       "Attack         0.2472      0.041      6.051      0.000       0.167       0.327\n",
       "Defense        0.1001      0.045      2.201      0.028       0.011       0.190\n",
       "==============================================================================\n",
       "Omnibus:                      284.299   Durbin-Watson:                   2.006\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5870.841\n",
       "Skew:                           2.720   Prob(JB):                         0.00\n",
       "Kurtosis:                      20.963   Cond. No.                         343.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_spec3 = smf.ols(formula='HP ~ Attack + Defense', \n",
    "                      data=pokeaman_train)\n",
    "model3_fit = model_spec3.fit()\n",
    "model3_fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e70f70",
   "metadata": {},
   "source": [
    "We now create a fitted linear regression on the trained dataset and view the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac52c0b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.14771558304519894\n",
      "'Out of sample' R-squared: 0.21208501873920738\n"
     ]
    }
   ],
   "source": [
    "yhat_model3 = model3_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model3_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model3)[0,1]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a14712",
   "metadata": {},
   "source": [
    "We are now evaluating the performance of the linear regression model on both the training set (in-sample) and the test set (out-of-sample). In-sample means data that the model was trained on or has seen, while out-of-sample is about data that's new to the model aka never seen before. **Note that in-sample $R^2$ measures the model's ability to explain the variance of the data it was trained on, while out-of-sample measures the *model's predictive power* on unseen data.** *model3_fit.predict(pokeaman_test)* uses the fitted linear regression model to make predictions on the test set of pokeaman (the never seen before dataset). This is stored in *yhat_model3* which is an array of predicted values for the HP (outcome variable) based on the test set's Attack and Defense features. *y* refers to the actual HP values of the test dataset, which we call our true value since the 'sample' is the predicted values and we're trying to see how well the predicted values match our true values, basically how well the model did. Our in-sample $R^2$ tells us the proportion of variance in HP that can be explained by the trained model. The out-of-sample $R^2$ is calculated using *np.corrcoef* because we want the model's predictive power so we compute the correlation coefficient between our true HP (y) and the predicted HP (yhat_model3). Squaring this gets us our out-of-sample $R^2$ which represents how well the model's predictions on the test set match the actual outcomes. Out-of-sample $R^2$ helps evaluate the generalizability of the model, so a value close to 1 indicates that the model is performing well on unseen data while a value closer to 0 suggests it's doing poorly on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cfa038c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.467</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.369</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   4.764</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 13 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>4.23e-21</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:53:04</td>     <th>  Log-Likelihood:    </th> <td> -1738.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   3603.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   337</td>      <th>  BIC:               </th> <td>   3855.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    62</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                                  <td></td>                                    <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                                                        <td>  521.5715</td> <td>  130.273</td> <td>    4.004</td> <td> 0.000</td> <td>  265.322</td> <td>  777.821</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]</th>                                                <td>   -6.1179</td> <td>    2.846</td> <td>   -2.150</td> <td> 0.032</td> <td>  -11.716</td> <td>   -0.520</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack</th>                                                           <td>   -8.1938</td> <td>    2.329</td> <td>   -3.518</td> <td> 0.000</td> <td>  -12.775</td> <td>   -3.612</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Legendary[T.True]</th>                                         <td>-1224.9610</td> <td>  545.105</td> <td>   -2.247</td> <td> 0.025</td> <td>-2297.199</td> <td> -152.723</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense</th>                                                          <td>   -6.1989</td> <td>    2.174</td> <td>   -2.851</td> <td> 0.005</td> <td>  -10.475</td> <td>   -1.923</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Legendary[T.True]</th>                                        <td> -102.4030</td> <td>   96.565</td> <td>   -1.060</td> <td> 0.290</td> <td> -292.350</td> <td>   87.544</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense</th>                                                   <td>    0.0985</td> <td>    0.033</td> <td>    2.982</td> <td> 0.003</td> <td>    0.034</td> <td>    0.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Legendary[T.True]</th>                                 <td>   14.6361</td> <td>    6.267</td> <td>    2.336</td> <td> 0.020</td> <td>    2.310</td> <td>   26.963</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed</th>                                                            <td>   -7.2261</td> <td>    2.178</td> <td>   -3.318</td> <td> 0.001</td> <td>  -11.511</td> <td>   -2.942</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Legendary[T.True]</th>                                          <td>  704.8798</td> <td>  337.855</td> <td>    2.086</td> <td> 0.038</td> <td>   40.309</td> <td> 1369.450</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed</th>                                                     <td>    0.1264</td> <td>    0.038</td> <td>    3.351</td> <td> 0.001</td> <td>    0.052</td> <td>    0.201</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Legendary[T.True]</th>                                   <td>    5.8648</td> <td>    2.692</td> <td>    2.179</td> <td> 0.030</td> <td>    0.570</td> <td>   11.160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed</th>                                                    <td>    0.1026</td> <td>    0.039</td> <td>    2.634</td> <td> 0.009</td> <td>    0.026</td> <td>    0.179</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Legendary[T.True]</th>                                  <td>   -6.9266</td> <td>    3.465</td> <td>   -1.999</td> <td> 0.046</td> <td>  -13.742</td> <td>   -0.111</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed</th>                                             <td>   -0.0016</td> <td>    0.001</td> <td>   -2.837</td> <td> 0.005</td> <td>   -0.003</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Legendary[T.True]</th>                           <td>   -0.0743</td> <td>    0.030</td> <td>   -2.477</td> <td> 0.014</td> <td>   -0.133</td> <td>   -0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\")</th>                                                     <td>   -5.3982</td> <td>    1.938</td> <td>   -2.785</td> <td> 0.006</td> <td>   -9.211</td> <td>   -1.586</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]:Q(\"Sp. Def\")</th>                                   <td> -282.2496</td> <td>  126.835</td> <td>   -2.225</td> <td> 0.027</td> <td> -531.738</td> <td>  -32.761</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Q(\"Sp. Def\")</th>                                              <td>    0.1094</td> <td>    0.034</td> <td>    3.233</td> <td> 0.001</td> <td>    0.043</td> <td>    0.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Legendary[T.True]:Q(\"Sp. Def\")</th>                            <td>   12.6503</td> <td>    5.851</td> <td>    2.162</td> <td> 0.031</td> <td>    1.141</td> <td>   24.160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Q(\"Sp. Def\")</th>                                             <td>    0.0628</td> <td>    0.028</td> <td>    2.247</td> <td> 0.025</td> <td>    0.008</td> <td>    0.118</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Legendary[T.True]:Q(\"Sp. Def\")</th>                           <td>    3.3949</td> <td>    1.783</td> <td>    1.904</td> <td> 0.058</td> <td>   -0.112</td> <td>    6.902</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Q(\"Sp. Def\")</th>                                      <td>   -0.0012</td> <td>    0.000</td> <td>   -2.730</td> <td> 0.007</td> <td>   -0.002</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\")</th>                    <td>   -0.1456</td> <td>    0.065</td> <td>   -2.253</td> <td> 0.025</td> <td>   -0.273</td> <td>   -0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Q(\"Sp. Def\")</th>                                               <td>    0.0624</td> <td>    0.031</td> <td>    2.027</td> <td> 0.043</td> <td>    0.002</td> <td>    0.123</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Legendary[T.True]:Q(\"Sp. Def\")</th>                             <td>   -3.2219</td> <td>    1.983</td> <td>   -1.625</td> <td> 0.105</td> <td>   -7.122</td> <td>    0.678</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Q(\"Sp. Def\")</th>                                        <td>   -0.0014</td> <td>    0.001</td> <td>   -2.732</td> <td> 0.007</td> <td>   -0.002</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\")</th>                      <td>   -0.0695</td> <td>    0.033</td> <td>   -2.100</td> <td> 0.036</td> <td>   -0.135</td> <td>   -0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Q(\"Sp. Def\")</th>                                       <td>   -0.0008</td> <td>    0.000</td> <td>   -1.743</td> <td> 0.082</td> <td>   -0.002</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")</th>                     <td>    0.0334</td> <td>    0.021</td> <td>    1.569</td> <td> 0.117</td> <td>   -0.008</td> <td>    0.075</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Q(\"Sp. Def\")</th>                                <td> 1.629e-05</td> <td> 6.92e-06</td> <td>    2.355</td> <td> 0.019</td> <td> 2.68e-06</td> <td> 2.99e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")</th>              <td>    0.0008</td> <td>    0.000</td> <td>    2.433</td> <td> 0.015</td> <td>    0.000</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Atk\")</th>                                                     <td>   -8.3636</td> <td>    2.346</td> <td>   -3.565</td> <td> 0.000</td> <td>  -12.978</td> <td>   -3.749</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]:Q(\"Sp. Atk\")</th>                                   <td>  850.5436</td> <td>  385.064</td> <td>    2.209</td> <td> 0.028</td> <td>   93.112</td> <td> 1607.975</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Q(\"Sp. Atk\")</th>                                              <td>    0.1388</td> <td>    0.040</td> <td>    3.500</td> <td> 0.001</td> <td>    0.061</td> <td>    0.217</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Legendary[T.True]:Q(\"Sp. Atk\")</th>                            <td>    2.1809</td> <td>    1.136</td> <td>    1.920</td> <td> 0.056</td> <td>   -0.054</td> <td>    4.416</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Q(\"Sp. Atk\")</th>                                             <td>    0.0831</td> <td>    0.038</td> <td>    2.162</td> <td> 0.031</td> <td>    0.007</td> <td>    0.159</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Legendary[T.True]:Q(\"Sp. Atk\")</th>                           <td>   -7.3121</td> <td>    3.376</td> <td>   -2.166</td> <td> 0.031</td> <td>  -13.953</td> <td>   -0.671</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Q(\"Sp. Atk\")</th>                                      <td>   -0.0014</td> <td>    0.001</td> <td>   -2.480</td> <td> 0.014</td> <td>   -0.003</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Legendary[T.True]:Q(\"Sp. Atk\")</th>                    <td>   -0.0434</td> <td>    0.022</td> <td>   -2.010</td> <td> 0.045</td> <td>   -0.086</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Q(\"Sp. Atk\")</th>                                               <td>    0.1011</td> <td>    0.035</td> <td>    2.872</td> <td> 0.004</td> <td>    0.032</td> <td>    0.170</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Legendary[T.True]:Q(\"Sp. Atk\")</th>                             <td>  -12.6343</td> <td>    5.613</td> <td>   -2.251</td> <td> 0.025</td> <td>  -23.674</td> <td>   -1.594</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Q(\"Sp. Atk\")</th>                                        <td>   -0.0018</td> <td>    0.001</td> <td>   -3.102</td> <td> 0.002</td> <td>   -0.003</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Legendary[T.True]:Q(\"Sp. Atk\")</th>                      <td>    0.0151</td> <td>    0.009</td> <td>    1.609</td> <td> 0.109</td> <td>   -0.003</td> <td>    0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Q(\"Sp. Atk\")</th>                                       <td>   -0.0012</td> <td>    0.001</td> <td>   -1.860</td> <td> 0.064</td> <td>   -0.002</td> <td> 6.62e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")</th>                     <td>    0.1210</td> <td>    0.054</td> <td>    2.260</td> <td> 0.024</td> <td>    0.016</td> <td>    0.226</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Q(\"Sp. Atk\")</th>                                <td> 2.125e-05</td> <td>  9.1e-06</td> <td>    2.334</td> <td> 0.020</td> <td> 3.34e-06</td> <td> 3.92e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")</th>              <td> 6.438e-06</td> <td> 7.69e-05</td> <td>    0.084</td> <td> 0.933</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                                        <td>    0.1265</td> <td>    0.033</td> <td>    3.821</td> <td> 0.000</td> <td>    0.061</td> <td>    0.192</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                      <td>   -5.0544</td> <td>    2.506</td> <td>   -2.017</td> <td> 0.044</td> <td>   -9.983</td> <td>   -0.126</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                                 <td>   -0.0021</td> <td>    0.001</td> <td>   -3.606</td> <td> 0.000</td> <td>   -0.003</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>               <td>   -0.0346</td> <td>    0.017</td> <td>   -1.992</td> <td> 0.047</td> <td>   -0.069</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                                <td>   -0.0012</td> <td>    0.000</td> <td>   -2.406</td> <td> 0.017</td> <td>   -0.002</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>              <td>    0.0446</td> <td>    0.025</td> <td>    1.794</td> <td> 0.074</td> <td>   -0.004</td> <td>    0.093</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                         <td> 1.973e-05</td> <td> 7.28e-06</td> <td>    2.710</td> <td> 0.007</td> <td> 5.41e-06</td> <td>  3.4e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>       <td>    0.0005</td> <td>    0.000</td> <td>    1.957</td> <td> 0.051</td> <td>-2.56e-06</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                                  <td>   -0.0013</td> <td>    0.000</td> <td>   -2.740</td> <td> 0.006</td> <td>   -0.002</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                <td>    0.0841</td> <td>    0.040</td> <td>    2.125</td> <td> 0.034</td> <td>    0.006</td> <td>    0.162</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                           <td> 2.379e-05</td> <td> 7.85e-06</td> <td>    3.030</td> <td> 0.003</td> <td> 8.34e-06</td> <td> 3.92e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>         <td> 2.864e-05</td> <td> 7.73e-05</td> <td>    0.370</td> <td> 0.711</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                          <td> 1.284e-05</td> <td> 7.46e-06</td> <td>    1.721</td> <td> 0.086</td> <td>-1.83e-06</td> <td> 2.75e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>        <td>   -0.0008</td> <td>    0.000</td> <td>   -2.085</td> <td> 0.038</td> <td>   -0.002</td> <td>-4.68e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                   <td> -2.53e-07</td> <td>  1.1e-07</td> <td>   -2.292</td> <td> 0.023</td> <td> -4.7e-07</td> <td>-3.59e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th> <td>-1.425e-06</td> <td> 1.14e-06</td> <td>   -1.249</td> <td> 0.212</td> <td>-3.67e-06</td> <td> 8.19e-07</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>214.307</td> <th>  Durbin-Watson:     </th> <td>   1.992</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2354.664</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.026</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>14.174</td>  <th>  Cond. No.          </th> <td>1.20e+16</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.2e+16. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                                                   &        HP        & \\textbf{  R-squared:         } &     0.467   \\\\\n",
       "\\textbf{Model:}                                                           &       OLS        & \\textbf{  Adj. R-squared:    } &     0.369   \\\\\n",
       "\\textbf{Method:}                                                          &  Least Squares   & \\textbf{  F-statistic:       } &     4.764   \\\\\n",
       "\\textbf{Date:}                                                            & Wed, 13 Nov 2024 & \\textbf{  Prob (F-statistic):} &  4.23e-21   \\\\\n",
       "\\textbf{Time:}                                                            &     21:53:04     & \\textbf{  Log-Likelihood:    } &   -1738.6   \\\\\n",
       "\\textbf{No. Observations:}                                                &         400      & \\textbf{  AIC:               } &     3603.   \\\\\n",
       "\\textbf{Df Residuals:}                                                    &         337      & \\textbf{  BIC:               } &     3855.   \\\\\n",
       "\\textbf{Df Model:}                                                        &          62      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}                                                 &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                                                          & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                                                        &     521.5715  &      130.273     &     4.004  &         0.000        &      265.322    &      777.821     \\\\\n",
       "\\textbf{Legendary[T.True]}                                                &      -6.1179  &        2.846     &    -2.150  &         0.032        &      -11.716    &       -0.520     \\\\\n",
       "\\textbf{Attack}                                                           &      -8.1938  &        2.329     &    -3.518  &         0.000        &      -12.775    &       -3.612     \\\\\n",
       "\\textbf{Attack:Legendary[T.True]}                                         &   -1224.9610  &      545.105     &    -2.247  &         0.025        &    -2297.199    &     -152.723     \\\\\n",
       "\\textbf{Defense}                                                          &      -6.1989  &        2.174     &    -2.851  &         0.005        &      -10.475    &       -1.923     \\\\\n",
       "\\textbf{Defense:Legendary[T.True]}                                        &    -102.4030  &       96.565     &    -1.060  &         0.290        &     -292.350    &       87.544     \\\\\n",
       "\\textbf{Attack:Defense}                                                   &       0.0985  &        0.033     &     2.982  &         0.003        &        0.034    &        0.164     \\\\\n",
       "\\textbf{Attack:Defense:Legendary[T.True]}                                 &      14.6361  &        6.267     &     2.336  &         0.020        &        2.310    &       26.963     \\\\\n",
       "\\textbf{Speed}                                                            &      -7.2261  &        2.178     &    -3.318  &         0.001        &      -11.511    &       -2.942     \\\\\n",
       "\\textbf{Speed:Legendary[T.True]}                                          &     704.8798  &      337.855     &     2.086  &         0.038        &       40.309    &     1369.450     \\\\\n",
       "\\textbf{Attack:Speed}                                                     &       0.1264  &        0.038     &     3.351  &         0.001        &        0.052    &        0.201     \\\\\n",
       "\\textbf{Attack:Speed:Legendary[T.True]}                                   &       5.8648  &        2.692     &     2.179  &         0.030        &        0.570    &       11.160     \\\\\n",
       "\\textbf{Defense:Speed}                                                    &       0.1026  &        0.039     &     2.634  &         0.009        &        0.026    &        0.179     \\\\\n",
       "\\textbf{Defense:Speed:Legendary[T.True]}                                  &      -6.9266  &        3.465     &    -1.999  &         0.046        &      -13.742    &       -0.111     \\\\\n",
       "\\textbf{Attack:Defense:Speed}                                             &      -0.0016  &        0.001     &    -2.837  &         0.005        &       -0.003    &       -0.001     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Legendary[T.True]}                           &      -0.0743  &        0.030     &    -2.477  &         0.014        &       -0.133    &       -0.015     \\\\\n",
       "\\textbf{Q(\"Sp. Def\")}                                                     &      -5.3982  &        1.938     &    -2.785  &         0.006        &       -9.211    &       -1.586     \\\\\n",
       "\\textbf{Legendary[T.True]:Q(\"Sp. Def\")}                                   &    -282.2496  &      126.835     &    -2.225  &         0.027        &     -531.738    &      -32.761     \\\\\n",
       "\\textbf{Attack:Q(\"Sp. Def\")}                                              &       0.1094  &        0.034     &     3.233  &         0.001        &        0.043    &        0.176     \\\\\n",
       "\\textbf{Attack:Legendary[T.True]:Q(\"Sp. Def\")}                            &      12.6503  &        5.851     &     2.162  &         0.031        &        1.141    &       24.160     \\\\\n",
       "\\textbf{Defense:Q(\"Sp. Def\")}                                             &       0.0628  &        0.028     &     2.247  &         0.025        &        0.008    &        0.118     \\\\\n",
       "\\textbf{Defense:Legendary[T.True]:Q(\"Sp. Def\")}                           &       3.3949  &        1.783     &     1.904  &         0.058        &       -0.112    &        6.902     \\\\\n",
       "\\textbf{Attack:Defense:Q(\"Sp. Def\")}                                      &      -0.0012  &        0.000     &    -2.730  &         0.007        &       -0.002    &       -0.000     \\\\\n",
       "\\textbf{Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\")}                    &      -0.1456  &        0.065     &    -2.253  &         0.025        &       -0.273    &       -0.018     \\\\\n",
       "\\textbf{Speed:Q(\"Sp. Def\")}                                               &       0.0624  &        0.031     &     2.027  &         0.043        &        0.002    &        0.123     \\\\\n",
       "\\textbf{Speed:Legendary[T.True]:Q(\"Sp. Def\")}                             &      -3.2219  &        1.983     &    -1.625  &         0.105        &       -7.122    &        0.678     \\\\\n",
       "\\textbf{Attack:Speed:Q(\"Sp. Def\")}                                        &      -0.0014  &        0.001     &    -2.732  &         0.007        &       -0.002    &       -0.000     \\\\\n",
       "\\textbf{Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\")}                      &      -0.0695  &        0.033     &    -2.100  &         0.036        &       -0.135    &       -0.004     \\\\\n",
       "\\textbf{Defense:Speed:Q(\"Sp. Def\")}                                       &      -0.0008  &        0.000     &    -1.743  &         0.082        &       -0.002    &        0.000     \\\\\n",
       "\\textbf{Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")}                     &       0.0334  &        0.021     &     1.569  &         0.117        &       -0.008    &        0.075     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Q(\"Sp. Def\")}                                &    1.629e-05  &     6.92e-06     &     2.355  &         0.019        &     2.68e-06    &     2.99e-05     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")}              &       0.0008  &        0.000     &     2.433  &         0.015        &        0.000    &        0.001     \\\\\n",
       "\\textbf{Q(\"Sp. Atk\")}                                                     &      -8.3636  &        2.346     &    -3.565  &         0.000        &      -12.978    &       -3.749     \\\\\n",
       "\\textbf{Legendary[T.True]:Q(\"Sp. Atk\")}                                   &     850.5436  &      385.064     &     2.209  &         0.028        &       93.112    &     1607.975     \\\\\n",
       "\\textbf{Attack:Q(\"Sp. Atk\")}                                              &       0.1388  &        0.040     &     3.500  &         0.001        &        0.061    &        0.217     \\\\\n",
       "\\textbf{Attack:Legendary[T.True]:Q(\"Sp. Atk\")}                            &       2.1809  &        1.136     &     1.920  &         0.056        &       -0.054    &        4.416     \\\\\n",
       "\\textbf{Defense:Q(\"Sp. Atk\")}                                             &       0.0831  &        0.038     &     2.162  &         0.031        &        0.007    &        0.159     \\\\\n",
       "\\textbf{Defense:Legendary[T.True]:Q(\"Sp. Atk\")}                           &      -7.3121  &        3.376     &    -2.166  &         0.031        &      -13.953    &       -0.671     \\\\\n",
       "\\textbf{Attack:Defense:Q(\"Sp. Atk\")}                                      &      -0.0014  &        0.001     &    -2.480  &         0.014        &       -0.003    &       -0.000     \\\\\n",
       "\\textbf{Attack:Defense:Legendary[T.True]:Q(\"Sp. Atk\")}                    &      -0.0434  &        0.022     &    -2.010  &         0.045        &       -0.086    &       -0.001     \\\\\n",
       "\\textbf{Speed:Q(\"Sp. Atk\")}                                               &       0.1011  &        0.035     &     2.872  &         0.004        &        0.032    &        0.170     \\\\\n",
       "\\textbf{Speed:Legendary[T.True]:Q(\"Sp. Atk\")}                             &     -12.6343  &        5.613     &    -2.251  &         0.025        &      -23.674    &       -1.594     \\\\\n",
       "\\textbf{Attack:Speed:Q(\"Sp. Atk\")}                                        &      -0.0018  &        0.001     &    -3.102  &         0.002        &       -0.003    &       -0.001     \\\\\n",
       "\\textbf{Attack:Speed:Legendary[T.True]:Q(\"Sp. Atk\")}                      &       0.0151  &        0.009     &     1.609  &         0.109        &       -0.003    &        0.034     \\\\\n",
       "\\textbf{Defense:Speed:Q(\"Sp. Atk\")}                                       &      -0.0012  &        0.001     &    -1.860  &         0.064        &       -0.002    &     6.62e-05     \\\\\n",
       "\\textbf{Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")}                     &       0.1210  &        0.054     &     2.260  &         0.024        &        0.016    &        0.226     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Q(\"Sp. Atk\")}                                &    2.125e-05  &      9.1e-06     &     2.334  &         0.020        &     3.34e-06    &     3.92e-05     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")}              &    6.438e-06  &     7.69e-05     &     0.084  &         0.933        &       -0.000    &        0.000     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                                        &       0.1265  &        0.033     &     3.821  &         0.000        &        0.061    &        0.192     \\\\\n",
       "\\textbf{Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                      &      -5.0544  &        2.506     &    -2.017  &         0.044        &       -9.983    &       -0.126     \\\\\n",
       "\\textbf{Attack:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                                 &      -0.0021  &        0.001     &    -3.606  &         0.000        &       -0.003    &       -0.001     \\\\\n",
       "\\textbf{Attack:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}               &      -0.0346  &        0.017     &    -1.992  &         0.047        &       -0.069    &       -0.000     \\\\\n",
       "\\textbf{Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                                &      -0.0012  &        0.000     &    -2.406  &         0.017        &       -0.002    &       -0.000     \\\\\n",
       "\\textbf{Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}              &       0.0446  &        0.025     &     1.794  &         0.074        &       -0.004    &        0.093     \\\\\n",
       "\\textbf{Attack:Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                         &    1.973e-05  &     7.28e-06     &     2.710  &         0.007        &     5.41e-06    &      3.4e-05     \\\\\n",
       "\\textbf{Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}       &       0.0005  &        0.000     &     1.957  &         0.051        &    -2.56e-06    &        0.001     \\\\\n",
       "\\textbf{Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                                  &      -0.0013  &        0.000     &    -2.740  &         0.006        &       -0.002    &       -0.000     \\\\\n",
       "\\textbf{Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                &       0.0841  &        0.040     &     2.125  &         0.034        &        0.006    &        0.162     \\\\\n",
       "\\textbf{Attack:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                           &    2.379e-05  &     7.85e-06     &     3.030  &         0.003        &     8.34e-06    &     3.92e-05     \\\\\n",
       "\\textbf{Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}         &    2.864e-05  &     7.73e-05     &     0.370  &         0.711        &       -0.000    &        0.000     \\\\\n",
       "\\textbf{Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                          &    1.284e-05  &     7.46e-06     &     1.721  &         0.086        &    -1.83e-06    &     2.75e-05     \\\\\n",
       "\\textbf{Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}        &      -0.0008  &        0.000     &    -2.085  &         0.038        &       -0.002    &    -4.68e-05     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                   &    -2.53e-07  &      1.1e-07     &    -2.292  &         0.023        &     -4.7e-07    &    -3.59e-08     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")} &   -1.425e-06  &     1.14e-06     &    -1.249  &         0.212        &    -3.67e-06    &     8.19e-07     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 214.307 & \\textbf{  Durbin-Watson:     } &    1.992  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 2354.664  \\\\\n",
       "\\textbf{Skew:}          &   2.026 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  14.174 & \\textbf{  Cond. No.          } & 1.20e+16  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 1.2e+16. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.467\n",
       "Model:                            OLS   Adj. R-squared:                  0.369\n",
       "Method:                 Least Squares   F-statistic:                     4.764\n",
       "Date:                Wed, 13 Nov 2024   Prob (F-statistic):           4.23e-21\n",
       "Time:                        21:53:04   Log-Likelihood:                -1738.6\n",
       "No. Observations:                 400   AIC:                             3603.\n",
       "Df Residuals:                     337   BIC:                             3855.\n",
       "Df Model:                          62                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================================================================\n",
       "                                                                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------------------------------------------------------\n",
       "Intercept                                                          521.5715    130.273      4.004      0.000     265.322     777.821\n",
       "Legendary[T.True]                                                   -6.1179      2.846     -2.150      0.032     -11.716      -0.520\n",
       "Attack                                                              -8.1938      2.329     -3.518      0.000     -12.775      -3.612\n",
       "Attack:Legendary[T.True]                                         -1224.9610    545.105     -2.247      0.025   -2297.199    -152.723\n",
       "Defense                                                             -6.1989      2.174     -2.851      0.005     -10.475      -1.923\n",
       "Defense:Legendary[T.True]                                         -102.4030     96.565     -1.060      0.290    -292.350      87.544\n",
       "Attack:Defense                                                       0.0985      0.033      2.982      0.003       0.034       0.164\n",
       "Attack:Defense:Legendary[T.True]                                    14.6361      6.267      2.336      0.020       2.310      26.963\n",
       "Speed                                                               -7.2261      2.178     -3.318      0.001     -11.511      -2.942\n",
       "Speed:Legendary[T.True]                                            704.8798    337.855      2.086      0.038      40.309    1369.450\n",
       "Attack:Speed                                                         0.1264      0.038      3.351      0.001       0.052       0.201\n",
       "Attack:Speed:Legendary[T.True]                                       5.8648      2.692      2.179      0.030       0.570      11.160\n",
       "Defense:Speed                                                        0.1026      0.039      2.634      0.009       0.026       0.179\n",
       "Defense:Speed:Legendary[T.True]                                     -6.9266      3.465     -1.999      0.046     -13.742      -0.111\n",
       "Attack:Defense:Speed                                                -0.0016      0.001     -2.837      0.005      -0.003      -0.001\n",
       "Attack:Defense:Speed:Legendary[T.True]                              -0.0743      0.030     -2.477      0.014      -0.133      -0.015\n",
       "Q(\"Sp. Def\")                                                        -5.3982      1.938     -2.785      0.006      -9.211      -1.586\n",
       "Legendary[T.True]:Q(\"Sp. Def\")                                    -282.2496    126.835     -2.225      0.027    -531.738     -32.761\n",
       "Attack:Q(\"Sp. Def\")                                                  0.1094      0.034      3.233      0.001       0.043       0.176\n",
       "Attack:Legendary[T.True]:Q(\"Sp. Def\")                               12.6503      5.851      2.162      0.031       1.141      24.160\n",
       "Defense:Q(\"Sp. Def\")                                                 0.0628      0.028      2.247      0.025       0.008       0.118\n",
       "Defense:Legendary[T.True]:Q(\"Sp. Def\")                               3.3949      1.783      1.904      0.058      -0.112       6.902\n",
       "Attack:Defense:Q(\"Sp. Def\")                                         -0.0012      0.000     -2.730      0.007      -0.002      -0.000\n",
       "Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\")                       -0.1456      0.065     -2.253      0.025      -0.273      -0.018\n",
       "Speed:Q(\"Sp. Def\")                                                   0.0624      0.031      2.027      0.043       0.002       0.123\n",
       "Speed:Legendary[T.True]:Q(\"Sp. Def\")                                -3.2219      1.983     -1.625      0.105      -7.122       0.678\n",
       "Attack:Speed:Q(\"Sp. Def\")                                           -0.0014      0.001     -2.732      0.007      -0.002      -0.000\n",
       "Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\")                         -0.0695      0.033     -2.100      0.036      -0.135      -0.004\n",
       "Defense:Speed:Q(\"Sp. Def\")                                          -0.0008      0.000     -1.743      0.082      -0.002       0.000\n",
       "Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")                         0.0334      0.021      1.569      0.117      -0.008       0.075\n",
       "Attack:Defense:Speed:Q(\"Sp. Def\")                                 1.629e-05   6.92e-06      2.355      0.019    2.68e-06    2.99e-05\n",
       "Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")                  0.0008      0.000      2.433      0.015       0.000       0.001\n",
       "Q(\"Sp. Atk\")                                                        -8.3636      2.346     -3.565      0.000     -12.978      -3.749\n",
       "Legendary[T.True]:Q(\"Sp. Atk\")                                     850.5436    385.064      2.209      0.028      93.112    1607.975\n",
       "Attack:Q(\"Sp. Atk\")                                                  0.1388      0.040      3.500      0.001       0.061       0.217\n",
       "Attack:Legendary[T.True]:Q(\"Sp. Atk\")                                2.1809      1.136      1.920      0.056      -0.054       4.416\n",
       "Defense:Q(\"Sp. Atk\")                                                 0.0831      0.038      2.162      0.031       0.007       0.159\n",
       "Defense:Legendary[T.True]:Q(\"Sp. Atk\")                              -7.3121      3.376     -2.166      0.031     -13.953      -0.671\n",
       "Attack:Defense:Q(\"Sp. Atk\")                                         -0.0014      0.001     -2.480      0.014      -0.003      -0.000\n",
       "Attack:Defense:Legendary[T.True]:Q(\"Sp. Atk\")                       -0.0434      0.022     -2.010      0.045      -0.086      -0.001\n",
       "Speed:Q(\"Sp. Atk\")                                                   0.1011      0.035      2.872      0.004       0.032       0.170\n",
       "Speed:Legendary[T.True]:Q(\"Sp. Atk\")                               -12.6343      5.613     -2.251      0.025     -23.674      -1.594\n",
       "Attack:Speed:Q(\"Sp. Atk\")                                           -0.0018      0.001     -3.102      0.002      -0.003      -0.001\n",
       "Attack:Speed:Legendary[T.True]:Q(\"Sp. Atk\")                          0.0151      0.009      1.609      0.109      -0.003       0.034\n",
       "Defense:Speed:Q(\"Sp. Atk\")                                          -0.0012      0.001     -1.860      0.064      -0.002    6.62e-05\n",
       "Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")                         0.1210      0.054      2.260      0.024       0.016       0.226\n",
       "Attack:Defense:Speed:Q(\"Sp. Atk\")                                 2.125e-05    9.1e-06      2.334      0.020    3.34e-06    3.92e-05\n",
       "Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")               6.438e-06   7.69e-05      0.084      0.933      -0.000       0.000\n",
       "Q(\"Sp. Def\"):Q(\"Sp. Atk\")                                            0.1265      0.033      3.821      0.000       0.061       0.192\n",
       "Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                         -5.0544      2.506     -2.017      0.044      -9.983      -0.126\n",
       "Attack:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                                    -0.0021      0.001     -3.606      0.000      -0.003      -0.001\n",
       "Attack:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                  -0.0346      0.017     -1.992      0.047      -0.069      -0.000\n",
       "Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                                   -0.0012      0.000     -2.406      0.017      -0.002      -0.000\n",
       "Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                  0.0446      0.025      1.794      0.074      -0.004       0.093\n",
       "Attack:Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                          1.973e-05   7.28e-06      2.710      0.007    5.41e-06     3.4e-05\n",
       "Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")           0.0005      0.000      1.957      0.051   -2.56e-06       0.001\n",
       "Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                                     -0.0013      0.000     -2.740      0.006      -0.002      -0.000\n",
       "Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                    0.0841      0.040      2.125      0.034       0.006       0.162\n",
       "Attack:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                            2.379e-05   7.85e-06      3.030      0.003    8.34e-06    3.92e-05\n",
       "Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")          2.864e-05   7.73e-05      0.370      0.711      -0.000       0.000\n",
       "Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                           1.284e-05   7.46e-06      1.721      0.086   -1.83e-06    2.75e-05\n",
       "Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")           -0.0008      0.000     -2.085      0.038      -0.002   -4.68e-05\n",
       "Attack:Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                    -2.53e-07    1.1e-07     -2.292      0.023    -4.7e-07   -3.59e-08\n",
       "Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\") -1.425e-06   1.14e-06     -1.249      0.212   -3.67e-06    8.19e-07\n",
       "==============================================================================\n",
       "Omnibus:                      214.307   Durbin-Watson:                   1.992\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2354.664\n",
       "Skew:                           2.026   Prob(JB):                         0.00\n",
       "Kurtosis:                      14.174   Cond. No.                     1.20e+16\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.2e+16. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4_linear_form = 'HP ~ Attack * Defense * Speed * Legendary'\n",
    "model4_linear_form += ' * Q(\"Sp. Def\") * Q(\"Sp. Atk\")'\n",
    "# DO NOT try adding '* C(Generation) * C(Q(\"Type 1\")) * C(Q(\"Type 2\"))'\n",
    "# That's 6*18*19 = 6*18*19 possible interaction combinations...\n",
    "# ...a huge number that will blow up your computer\n",
    "\n",
    "model4_spec = smf.ols(formula=model4_linear_form, data=pokeaman_train)\n",
    "model4_fit = model4_spec.fit()\n",
    "model4_fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd36ac2",
   "metadata": {},
   "source": [
    "Now we're doing a complex linear regression using interaction terms between multiple features of the trained dataset. *model4_linear_form* initializes a lienar regression model with an outcome variable HP and multiple predictor variables that are interacting with each other. This means that the model will not only consider the individual effects of each variable, but also their combined interaction (Attack * Defense, Attack * Speed, etc). By adding on Q(\"Sp. Def\") and Q(\"Sp. Atk\"), we add the interactions between Sp. Def and Sp. Attack, using Q() to account for column names that may contain spaces or special characters. The note just tells us that adding categorical variables would lead to an extremely large number of interaction terms. We then create a linear regression model on the new linear form and fit it (which estimates the coefficients for all the variables and interaction terms specified in the formula), then prints a summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61d225cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.46709442115833855\n",
      "'Out of sample' R-squared: 0.002485342598992873\n"
     ]
    }
   ],
   "source": [
    "yhat_model4 = model4_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model4_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model4)[0,1]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da85bc64",
   "metadata": {},
   "source": [
    "We are doing the same thing as in model 3 where we use the trained dataset to make predictions on the test dataset (data the trained model has never seen before), calculate the in-sample $R^2$ which tells us the proportion of variance in HP that can be explained by the trained model, and the out-of-sample $R^2$ tells us how closely the model predicted the values in relevance to the true values. An $R^2$ closer to 0 means the predictive model is weak on new data, but a value closer to 1 means the predictive model is strong and can accurately perform on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deecc5b7",
   "metadata": {},
   "source": [
    "### 6. Work with a ChatBot to understand how the *model4_linear_form* (*linear form* specification of  *model4*) creates new *predictor variables* as the columns of the so-called \"design matrix\" *model4_spec.exog* (*model4_spec.exog.shape*) used to predict the *outcome variable*  *model4_spec.endog* and why the so-called *multicollinearity* in this \"design matrix\" (observed in *np.corrcoef(model4_spec.exog)*) contribues to the lack of \"out of sample\" *generalization* of *predictions* from *model4_fit*; then, explain this consisely in your own works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66dd4227",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   34.40</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 13 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>1.66e-14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:53:04</td>     <th>  Log-Likelihood:    </th> <td> -1832.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   3671.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   397</td>      <th>  BIC:               </th> <td>   3683.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   42.5882</td> <td>    3.580</td> <td>   11.897</td> <td> 0.000</td> <td>   35.551</td> <td>   49.626</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack</th>    <td>    0.2472</td> <td>    0.041</td> <td>    6.051</td> <td> 0.000</td> <td>    0.167</td> <td>    0.327</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense</th>   <td>    0.1001</td> <td>    0.045</td> <td>    2.201</td> <td> 0.028</td> <td>    0.011</td> <td>    0.190</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>284.299</td> <th>  Durbin-Watson:     </th> <td>   2.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>5870.841</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.720</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>20.963</td>  <th>  Cond. No.          </th> <td>    343.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &        HP        & \\textbf{  R-squared:         } &     0.148   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.143   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     34.40   \\\\\n",
       "\\textbf{Date:}             & Wed, 13 Nov 2024 & \\textbf{  Prob (F-statistic):} &  1.66e-14   \\\\\n",
       "\\textbf{Time:}             &     21:53:04     & \\textbf{  Log-Likelihood:    } &   -1832.6   \\\\\n",
       "\\textbf{No. Observations:} &         400      & \\textbf{  AIC:               } &     3671.   \\\\\n",
       "\\textbf{Df Residuals:}     &         397      & \\textbf{  BIC:               } &     3683.   \\\\\n",
       "\\textbf{Df Model:}         &           2      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept} &      42.5882  &        3.580     &    11.897  &         0.000        &       35.551    &       49.626     \\\\\n",
       "\\textbf{Attack}    &       0.2472  &        0.041     &     6.051  &         0.000        &        0.167    &        0.327     \\\\\n",
       "\\textbf{Defense}   &       0.1001  &        0.045     &     2.201  &         0.028        &        0.011    &        0.190     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 284.299 & \\textbf{  Durbin-Watson:     } &    2.006  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 5870.841  \\\\\n",
       "\\textbf{Skew:}          &   2.720 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  20.963 & \\textbf{  Cond. No.          } &     343.  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.148\n",
       "Model:                            OLS   Adj. R-squared:                  0.143\n",
       "Method:                 Least Squares   F-statistic:                     34.40\n",
       "Date:                Wed, 13 Nov 2024   Prob (F-statistic):           1.66e-14\n",
       "Time:                        21:53:04   Log-Likelihood:                -1832.6\n",
       "No. Observations:                 400   AIC:                             3671.\n",
       "Df Residuals:                     397   BIC:                             3683.\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     42.5882      3.580     11.897      0.000      35.551      49.626\n",
       "Attack         0.2472      0.041      6.051      0.000       0.167       0.327\n",
       "Defense        0.1001      0.045      2.201      0.028       0.011       0.190\n",
       "==============================================================================\n",
       "Omnibus:                      284.299   Durbin-Watson:                   2.006\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5870.841\n",
       "Skew:                           2.720   Prob(JB):                         0.00\n",
       "Kurtosis:                      20.963   Cond. No.                         343.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"Cond. No.\" WAS 343.0 WITHOUT to centering and scaling\n",
    "model3_fit.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dedf01b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   34.40</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 13 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>1.66e-14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:53:04</td>     <th>  Log-Likelihood:    </th> <td> -1832.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   3671.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   397</td>      <th>  BIC:               </th> <td>   3683.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "             <td></td>               <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>              <td>   69.3025</td> <td>    1.186</td> <td>   58.439</td> <td> 0.000</td> <td>   66.971</td> <td>   71.634</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>scale(center(Attack))</th>  <td>    8.1099</td> <td>    1.340</td> <td>    6.051</td> <td> 0.000</td> <td>    5.475</td> <td>   10.745</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>scale(center(Defense))</th> <td>    2.9496</td> <td>    1.340</td> <td>    2.201</td> <td> 0.028</td> <td>    0.315</td> <td>    5.585</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>284.299</td> <th>  Durbin-Watson:     </th> <td>   2.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>5870.841</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.720</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>20.963</td>  <th>  Cond. No.          </th> <td>    1.66</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}         &        HP        & \\textbf{  R-squared:         } &     0.148   \\\\\n",
       "\\textbf{Model:}                 &       OLS        & \\textbf{  Adj. R-squared:    } &     0.143   \\\\\n",
       "\\textbf{Method:}                &  Least Squares   & \\textbf{  F-statistic:       } &     34.40   \\\\\n",
       "\\textbf{Date:}                  & Wed, 13 Nov 2024 & \\textbf{  Prob (F-statistic):} &  1.66e-14   \\\\\n",
       "\\textbf{Time:}                  &     21:53:04     & \\textbf{  Log-Likelihood:    } &   -1832.6   \\\\\n",
       "\\textbf{No. Observations:}      &         400      & \\textbf{  AIC:               } &     3671.   \\\\\n",
       "\\textbf{Df Residuals:}          &         397      & \\textbf{  BIC:               } &     3683.   \\\\\n",
       "\\textbf{Df Model:}              &           2      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}       &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}              &      69.3025  &        1.186     &    58.439  &         0.000        &       66.971    &       71.634     \\\\\n",
       "\\textbf{scale(center(Attack))}  &       8.1099  &        1.340     &     6.051  &         0.000        &        5.475    &       10.745     \\\\\n",
       "\\textbf{scale(center(Defense))} &       2.9496  &        1.340     &     2.201  &         0.028        &        0.315    &        5.585     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 284.299 & \\textbf{  Durbin-Watson:     } &    2.006  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 5870.841  \\\\\n",
       "\\textbf{Skew:}          &   2.720 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  20.963 & \\textbf{  Cond. No.          } &     1.66  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.148\n",
       "Model:                            OLS   Adj. R-squared:                  0.143\n",
       "Method:                 Least Squares   F-statistic:                     34.40\n",
       "Date:                Wed, 13 Nov 2024   Prob (F-statistic):           1.66e-14\n",
       "Time:                        21:53:04   Log-Likelihood:                -1832.6\n",
       "No. Observations:                 400   AIC:                             3671.\n",
       "Df Residuals:                     397   BIC:                             3683.\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==========================================================================================\n",
       "                             coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------------\n",
       "Intercept                 69.3025      1.186     58.439      0.000      66.971      71.634\n",
       "scale(center(Attack))      8.1099      1.340      6.051      0.000       5.475      10.745\n",
       "scale(center(Defense))     2.9496      1.340      2.201      0.028       0.315       5.585\n",
       "==============================================================================\n",
       "Omnibus:                      284.299   Durbin-Watson:                   2.006\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5870.841\n",
       "Skew:                           2.720   Prob(JB):                         0.00\n",
       "Kurtosis:                      20.963   Cond. No.                         1.66\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from patsy import center, scale\n",
    "\n",
    "model3_linear_form_center_scale = \\\n",
    "  'HP ~ scale(center(Attack)) + scale(center(Defense))' \n",
    "model_spec3_center_scale = smf.ols(formula=model3_linear_form_center_scale,\n",
    "                                   data=pokeaman_train)\n",
    "model3_center_scale_fit = model_spec3_center_scale.fit()\n",
    "model3_center_scale_fit.summary()\n",
    "# \"Cond. No.\" is NOW 1.66 due to centering and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cb64e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>214.307</td> <th>  Durbin-Watson:     </th> <td>   1.992</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2354.663</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.026</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>14.174</td>  <th>  Cond. No.          </th> <td>1.54e+16</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Omnibus:}       & 214.307 & \\textbf{  Durbin-Watson:     } &    1.992  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 2354.663  \\\\\n",
       "\\textbf{Skew:}          &   2.026 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  14.174 & \\textbf{  Cond. No.          } & 1.54e+16  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4_linear_form_CS = 'HP ~ scale(center(Attack)) * scale(center(Defense))'\n",
    "model4_linear_form_CS += ' * scale(center(Speed)) * Legendary' \n",
    "model4_linear_form_CS += ' * scale(center(Q(\"Sp. Def\"))) * scale(center(Q(\"Sp. Atk\")))'\n",
    "# Legendary is an indicator, so we don't center and scale that\n",
    "\n",
    "model4_CS_spec = smf.ols(formula=model4_linear_form_CS, data=pokeaman_train)\n",
    "model4_CS_fit = model4_CS_spec.fit()\n",
    "model4_CS_fit.summary().tables[-1]  # Cond. No. is 2,250,000,000,000,000\n",
    "\n",
    "# The condition number is still bad even after centering and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5f19d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>214.307</td> <th>  Durbin-Watson:     </th> <td>   1.992</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2354.664</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.026</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>14.174</td>  <th>  Cond. No.          </th> <td>1.20e+16</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Omnibus:}       & 214.307 & \\textbf{  Durbin-Watson:     } &    1.992  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 2354.664  \\\\\n",
       "\\textbf{Skew:}          &   2.026 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  14.174 & \\textbf{  Cond. No.          } & 1.20e+16  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just as the condition number was very bad to start with\n",
    "model4_fit.summary().tables[-1]  # Cond. No. is 12,000,000,000,000,000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed440c4",
   "metadata": {},
   "source": [
    "The design matrix *model4_spec.exog* is a mathematical representation of the predictors used in a regression model. When the model tries to find a relationship between the outcome and predictors and you use specify a formula in the statsmodels module for it, it automatically creates a matrix pf predictor variables including interactions and transformations based on the formula. Note that when you create a linear regression with interaction terms like Attack * Defense, the model will take in Attack and Defense as the main effects, and Attack * Defense as the interaction between them. This means that a new column for Attack * Defense is created with represents the interaction between the 2, and similarily for other interactions. The design matrix *model4_spec.exog* is constructed by combining all the main effects and interaction terms into a matrix of predictors that include individual columns for each predictor variable (Attack, Defense, Speed, Legendary) and additional columns for the interaction terms (Attack * Defense, Speed * Legendary). Basically, a design matrix is a numerical representation of the predictor variables where each row is an observation and each column is a predictor variable (including interactions). It's the input matrix used by the regression model to predict the outcome variable. \n",
    "\n",
    "Multicollinearity is when two or more predictors are similar, meaning they provide redundant or overlapping information. When this happens, it becomes difficult for the model to distinguish the individual effect of each predictor because they are not independent of each other. High multicollinearity makes the model's coefficient estimates unstable because small changes in the data can lead to large changes in the estimated coefficients. This makes the model sensitive to minor changes in the training dataset which makes it unreliable, so as the variance of the estimated coefficients increase, the less precise the predictions are. Specially, multicollinearity affects the model's ability to generalize to new data so while the model might fit the training data very well, it may not generalize well to unseen data because the highly correlated predictors might overfit to noise or specific patterns in the training set rather than capture true relationships happening between the variables.\n",
    "\n",
    "In essence, multicollinearity leads to overfitting where the model learns to fit the patterns and noise in the training data, but doesn't perform well on new data due to this. A high correlation between predictors in the design matrix (model4_spec.exog) means the model may not learn the true independent effect of each predictor, which contributes to the model's poor out-of-sample generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a577c7dd",
   "metadata": {},
   "source": [
    "### 7. Discuss with a ChatBot the rationale and principles by which *model5_linear_form* is  extended and developed from *model3_fit* and *model4_fit*; *model6_linear_form* is  extended and developed from *model5_linear_form*; *model7_linear_form* is  extended and developed from *model6_linear_form*; then, explain this breifly and consisely in your own words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5486d44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.392</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.313</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   4.948</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 13 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>9.48e-19</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:53:04</td>     <th>  Log-Likelihood:    </th> <td> -1765.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   3624.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   353</td>      <th>  BIC:               </th> <td>   3812.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    46</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "               <td></td>                 <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                  <td>   10.1046</td> <td>   14.957</td> <td>    0.676</td> <td> 0.500</td> <td>  -19.312</td> <td>   39.521</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]</th>          <td>   -3.2717</td> <td>    4.943</td> <td>   -0.662</td> <td> 0.508</td> <td>  -12.992</td> <td>    6.449</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.2]</th>         <td>    9.2938</td> <td>    4.015</td> <td>    2.315</td> <td> 0.021</td> <td>    1.398</td> <td>   17.189</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.3]</th>         <td>    2.3150</td> <td>    3.915</td> <td>    0.591</td> <td> 0.555</td> <td>   -5.385</td> <td>   10.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.4]</th>         <td>    4.8353</td> <td>    4.149</td> <td>    1.165</td> <td> 0.245</td> <td>   -3.325</td> <td>   12.995</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.5]</th>         <td>   11.4838</td> <td>    3.960</td> <td>    2.900</td> <td> 0.004</td> <td>    3.696</td> <td>   19.272</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.6]</th>         <td>    4.9206</td> <td>    4.746</td> <td>    1.037</td> <td> 0.300</td> <td>   -4.413</td> <td>   14.254</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Dark]</th>     <td>   -1.4155</td> <td>    6.936</td> <td>   -0.204</td> <td> 0.838</td> <td>  -15.057</td> <td>   12.226</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Dragon]</th>   <td>    0.8509</td> <td>    6.900</td> <td>    0.123</td> <td> 0.902</td> <td>  -12.720</td> <td>   14.422</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Electric]</th> <td>   -6.3641</td> <td>    6.537</td> <td>   -0.974</td> <td> 0.331</td> <td>  -19.220</td> <td>    6.491</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Fairy]</th>    <td>   -1.9486</td> <td>   10.124</td> <td>   -0.192</td> <td> 0.847</td> <td>  -21.859</td> <td>   17.962</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Fighting]</th> <td>    7.0308</td> <td>    7.432</td> <td>    0.946</td> <td> 0.345</td> <td>   -7.586</td> <td>   21.648</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Fire]</th>     <td>    3.0779</td> <td>    6.677</td> <td>    0.461</td> <td> 0.645</td> <td>  -10.055</td> <td>   16.210</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Flying]</th>   <td>   -2.1231</td> <td>   22.322</td> <td>   -0.095</td> <td> 0.924</td> <td>  -46.025</td> <td>   41.779</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Ghost]</th>    <td>    5.7343</td> <td>    8.488</td> <td>    0.676</td> <td> 0.500</td> <td>  -10.960</td> <td>   22.429</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Grass]</th>    <td>    3.3275</td> <td>    5.496</td> <td>    0.605</td> <td> 0.545</td> <td>   -7.481</td> <td>   14.136</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Ground]</th>   <td>    9.5118</td> <td>    7.076</td> <td>    1.344</td> <td> 0.180</td> <td>   -4.404</td> <td>   23.428</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Ice]</th>      <td>   -0.9313</td> <td>    7.717</td> <td>   -0.121</td> <td> 0.904</td> <td>  -16.108</td> <td>   14.246</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Normal]</th>   <td>   18.4816</td> <td>    5.312</td> <td>    3.479</td> <td> 0.001</td> <td>    8.034</td> <td>   28.929</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Poison]</th>   <td>    8.3411</td> <td>    7.735</td> <td>    1.078</td> <td> 0.282</td> <td>   -6.871</td> <td>   23.554</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Psychic]</th>  <td>    1.8061</td> <td>    6.164</td> <td>    0.293</td> <td> 0.770</td> <td>  -10.317</td> <td>   13.930</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Rock]</th>     <td>   -3.8558</td> <td>    6.503</td> <td>   -0.593</td> <td> 0.554</td> <td>  -16.645</td> <td>    8.933</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Steel]</th>    <td>   -4.0053</td> <td>    8.044</td> <td>   -0.498</td> <td> 0.619</td> <td>  -19.826</td> <td>   11.816</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Water]</th>    <td>    9.7988</td> <td>    5.166</td> <td>    1.897</td> <td> 0.059</td> <td>   -0.361</td> <td>   19.959</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Dark]</th>     <td>    5.8719</td> <td>   15.185</td> <td>    0.387</td> <td> 0.699</td> <td>  -23.993</td> <td>   35.737</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Dragon]</th>   <td>   13.2777</td> <td>   14.895</td> <td>    0.891</td> <td> 0.373</td> <td>  -16.016</td> <td>   42.571</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Electric]</th> <td>   14.3228</td> <td>   17.314</td> <td>    0.827</td> <td> 0.409</td> <td>  -19.728</td> <td>   48.374</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Fairy]</th>    <td>    2.8426</td> <td>   14.268</td> <td>    0.199</td> <td> 0.842</td> <td>  -25.218</td> <td>   30.903</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Fighting]</th> <td>    1.9741</td> <td>   14.089</td> <td>    0.140</td> <td> 0.889</td> <td>  -25.735</td> <td>   29.683</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Fire]</th>     <td>    0.2001</td> <td>   15.730</td> <td>    0.013</td> <td> 0.990</td> <td>  -30.736</td> <td>   31.136</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Flying]</th>   <td>    6.7292</td> <td>   13.581</td> <td>    0.495</td> <td> 0.621</td> <td>  -19.980</td> <td>   33.438</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Ghost]</th>    <td>  -10.9402</td> <td>   15.895</td> <td>   -0.688</td> <td> 0.492</td> <td>  -42.201</td> <td>   20.321</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Grass]</th>    <td>    2.5119</td> <td>   14.540</td> <td>    0.173</td> <td> 0.863</td> <td>  -26.084</td> <td>   31.108</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Ground]</th>   <td>   13.6042</td> <td>   13.655</td> <td>    0.996</td> <td> 0.320</td> <td>  -13.250</td> <td>   40.459</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Ice]</th>      <td>   19.7950</td> <td>   15.068</td> <td>    1.314</td> <td> 0.190</td> <td>   -9.840</td> <td>   49.430</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.None]</th>     <td>    7.6068</td> <td>   13.162</td> <td>    0.578</td> <td> 0.564</td> <td>  -18.279</td> <td>   33.493</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Normal]</th>   <td>   17.3191</td> <td>   17.764</td> <td>    0.975</td> <td> 0.330</td> <td>  -17.618</td> <td>   52.256</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Poison]</th>   <td>    0.7770</td> <td>   14.575</td> <td>    0.053</td> <td> 0.958</td> <td>  -27.887</td> <td>   29.441</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Psychic]</th>  <td>    4.2480</td> <td>   14.174</td> <td>    0.300</td> <td> 0.765</td> <td>  -23.628</td> <td>   32.124</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Rock]</th>     <td>    6.8858</td> <td>   16.221</td> <td>    0.424</td> <td> 0.671</td> <td>  -25.017</td> <td>   38.788</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Steel]</th>    <td>  -11.9623</td> <td>   14.973</td> <td>   -0.799</td> <td> 0.425</td> <td>  -41.409</td> <td>   17.485</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Water]</th>    <td>    5.8097</td> <td>   14.763</td> <td>    0.394</td> <td> 0.694</td> <td>  -23.225</td> <td>   34.845</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack</th>                     <td>    0.2508</td> <td>    0.051</td> <td>    4.940</td> <td> 0.000</td> <td>    0.151</td> <td>    0.351</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense</th>                    <td>   -0.0096</td> <td>    0.060</td> <td>   -0.160</td> <td> 0.873</td> <td>   -0.127</td> <td>    0.108</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed</th>                      <td>   -0.1538</td> <td>    0.051</td> <td>   -2.998</td> <td> 0.003</td> <td>   -0.255</td> <td>   -0.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\")</th>               <td>    0.3484</td> <td>    0.059</td> <td>    5.936</td> <td> 0.000</td> <td>    0.233</td> <td>    0.464</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Atk\")</th>               <td>    0.1298</td> <td>    0.051</td> <td>    2.525</td> <td> 0.012</td> <td>    0.029</td> <td>    0.231</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>286.476</td> <th>  Durbin-Watson:     </th> <td>   1.917</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>5187.327</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.807</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>19.725</td>  <th>  Cond. No.          </th> <td>9.21e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 9.21e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}             &        HP        & \\textbf{  R-squared:         } &     0.392   \\\\\n",
       "\\textbf{Model:}                     &       OLS        & \\textbf{  Adj. R-squared:    } &     0.313   \\\\\n",
       "\\textbf{Method:}                    &  Least Squares   & \\textbf{  F-statistic:       } &     4.948   \\\\\n",
       "\\textbf{Date:}                      & Wed, 13 Nov 2024 & \\textbf{  Prob (F-statistic):} &  9.48e-19   \\\\\n",
       "\\textbf{Time:}                      &     21:53:04     & \\textbf{  Log-Likelihood:    } &   -1765.0   \\\\\n",
       "\\textbf{No. Observations:}          &         400      & \\textbf{  AIC:               } &     3624.   \\\\\n",
       "\\textbf{Df Residuals:}              &         353      & \\textbf{  BIC:               } &     3812.   \\\\\n",
       "\\textbf{Df Model:}                  &          46      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}           &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                    & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                  &      10.1046  &       14.957     &     0.676  &         0.500        &      -19.312    &       39.521     \\\\\n",
       "\\textbf{Legendary[T.True]}          &      -3.2717  &        4.943     &    -0.662  &         0.508        &      -12.992    &        6.449     \\\\\n",
       "\\textbf{C(Generation)[T.2]}         &       9.2938  &        4.015     &     2.315  &         0.021        &        1.398    &       17.189     \\\\\n",
       "\\textbf{C(Generation)[T.3]}         &       2.3150  &        3.915     &     0.591  &         0.555        &       -5.385    &       10.015     \\\\\n",
       "\\textbf{C(Generation)[T.4]}         &       4.8353  &        4.149     &     1.165  &         0.245        &       -3.325    &       12.995     \\\\\n",
       "\\textbf{C(Generation)[T.5]}         &      11.4838  &        3.960     &     2.900  &         0.004        &        3.696    &       19.272     \\\\\n",
       "\\textbf{C(Generation)[T.6]}         &       4.9206  &        4.746     &     1.037  &         0.300        &       -4.413    &       14.254     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Dark]}     &      -1.4155  &        6.936     &    -0.204  &         0.838        &      -15.057    &       12.226     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Dragon]}   &       0.8509  &        6.900     &     0.123  &         0.902        &      -12.720    &       14.422     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Electric]} &      -6.3641  &        6.537     &    -0.974  &         0.331        &      -19.220    &        6.491     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Fairy]}    &      -1.9486  &       10.124     &    -0.192  &         0.847        &      -21.859    &       17.962     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Fighting]} &       7.0308  &        7.432     &     0.946  &         0.345        &       -7.586    &       21.648     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Fire]}     &       3.0779  &        6.677     &     0.461  &         0.645        &      -10.055    &       16.210     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Flying]}   &      -2.1231  &       22.322     &    -0.095  &         0.924        &      -46.025    &       41.779     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Ghost]}    &       5.7343  &        8.488     &     0.676  &         0.500        &      -10.960    &       22.429     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Grass]}    &       3.3275  &        5.496     &     0.605  &         0.545        &       -7.481    &       14.136     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Ground]}   &       9.5118  &        7.076     &     1.344  &         0.180        &       -4.404    &       23.428     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Ice]}      &      -0.9313  &        7.717     &    -0.121  &         0.904        &      -16.108    &       14.246     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Normal]}   &      18.4816  &        5.312     &     3.479  &         0.001        &        8.034    &       28.929     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Poison]}   &       8.3411  &        7.735     &     1.078  &         0.282        &       -6.871    &       23.554     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Psychic]}  &       1.8061  &        6.164     &     0.293  &         0.770        &      -10.317    &       13.930     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Rock]}     &      -3.8558  &        6.503     &    -0.593  &         0.554        &      -16.645    &        8.933     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Steel]}    &      -4.0053  &        8.044     &    -0.498  &         0.619        &      -19.826    &       11.816     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Water]}    &       9.7988  &        5.166     &     1.897  &         0.059        &       -0.361    &       19.959     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Dark]}     &       5.8719  &       15.185     &     0.387  &         0.699        &      -23.993    &       35.737     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Dragon]}   &      13.2777  &       14.895     &     0.891  &         0.373        &      -16.016    &       42.571     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Electric]} &      14.3228  &       17.314     &     0.827  &         0.409        &      -19.728    &       48.374     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Fairy]}    &       2.8426  &       14.268     &     0.199  &         0.842        &      -25.218    &       30.903     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Fighting]} &       1.9741  &       14.089     &     0.140  &         0.889        &      -25.735    &       29.683     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Fire]}     &       0.2001  &       15.730     &     0.013  &         0.990        &      -30.736    &       31.136     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Flying]}   &       6.7292  &       13.581     &     0.495  &         0.621        &      -19.980    &       33.438     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Ghost]}    &     -10.9402  &       15.895     &    -0.688  &         0.492        &      -42.201    &       20.321     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Grass]}    &       2.5119  &       14.540     &     0.173  &         0.863        &      -26.084    &       31.108     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Ground]}   &      13.6042  &       13.655     &     0.996  &         0.320        &      -13.250    &       40.459     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Ice]}      &      19.7950  &       15.068     &     1.314  &         0.190        &       -9.840    &       49.430     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.None]}     &       7.6068  &       13.162     &     0.578  &         0.564        &      -18.279    &       33.493     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Normal]}   &      17.3191  &       17.764     &     0.975  &         0.330        &      -17.618    &       52.256     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Poison]}   &       0.7770  &       14.575     &     0.053  &         0.958        &      -27.887    &       29.441     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Psychic]}  &       4.2480  &       14.174     &     0.300  &         0.765        &      -23.628    &       32.124     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Rock]}     &       6.8858  &       16.221     &     0.424  &         0.671        &      -25.017    &       38.788     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Steel]}    &     -11.9623  &       14.973     &    -0.799  &         0.425        &      -41.409    &       17.485     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Water]}    &       5.8097  &       14.763     &     0.394  &         0.694        &      -23.225    &       34.845     \\\\\n",
       "\\textbf{Attack}                     &       0.2508  &        0.051     &     4.940  &         0.000        &        0.151    &        0.351     \\\\\n",
       "\\textbf{Defense}                    &      -0.0096  &        0.060     &    -0.160  &         0.873        &       -0.127    &        0.108     \\\\\n",
       "\\textbf{Speed}                      &      -0.1538  &        0.051     &    -2.998  &         0.003        &       -0.255    &       -0.053     \\\\\n",
       "\\textbf{Q(\"Sp. Def\")}               &       0.3484  &        0.059     &     5.936  &         0.000        &        0.233    &        0.464     \\\\\n",
       "\\textbf{Q(\"Sp. Atk\")}               &       0.1298  &        0.051     &     2.525  &         0.012        &        0.029    &        0.231     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 286.476 & \\textbf{  Durbin-Watson:     } &    1.917  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 5187.327  \\\\\n",
       "\\textbf{Skew:}          &   2.807 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  19.725 & \\textbf{  Cond. No.          } & 9.21e+03  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 9.21e+03. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.392\n",
       "Model:                            OLS   Adj. R-squared:                  0.313\n",
       "Method:                 Least Squares   F-statistic:                     4.948\n",
       "Date:                Wed, 13 Nov 2024   Prob (F-statistic):           9.48e-19\n",
       "Time:                        21:53:04   Log-Likelihood:                -1765.0\n",
       "No. Observations:                 400   AIC:                             3624.\n",
       "Df Residuals:                     353   BIC:                             3812.\n",
       "Df Model:                          46                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================================\n",
       "                                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------------\n",
       "Intercept                     10.1046     14.957      0.676      0.500     -19.312      39.521\n",
       "Legendary[T.True]             -3.2717      4.943     -0.662      0.508     -12.992       6.449\n",
       "C(Generation)[T.2]             9.2938      4.015      2.315      0.021       1.398      17.189\n",
       "C(Generation)[T.3]             2.3150      3.915      0.591      0.555      -5.385      10.015\n",
       "C(Generation)[T.4]             4.8353      4.149      1.165      0.245      -3.325      12.995\n",
       "C(Generation)[T.5]            11.4838      3.960      2.900      0.004       3.696      19.272\n",
       "C(Generation)[T.6]             4.9206      4.746      1.037      0.300      -4.413      14.254\n",
       "C(Q(\"Type 1\"))[T.Dark]        -1.4155      6.936     -0.204      0.838     -15.057      12.226\n",
       "C(Q(\"Type 1\"))[T.Dragon]       0.8509      6.900      0.123      0.902     -12.720      14.422\n",
       "C(Q(\"Type 1\"))[T.Electric]    -6.3641      6.537     -0.974      0.331     -19.220       6.491\n",
       "C(Q(\"Type 1\"))[T.Fairy]       -1.9486     10.124     -0.192      0.847     -21.859      17.962\n",
       "C(Q(\"Type 1\"))[T.Fighting]     7.0308      7.432      0.946      0.345      -7.586      21.648\n",
       "C(Q(\"Type 1\"))[T.Fire]         3.0779      6.677      0.461      0.645     -10.055      16.210\n",
       "C(Q(\"Type 1\"))[T.Flying]      -2.1231     22.322     -0.095      0.924     -46.025      41.779\n",
       "C(Q(\"Type 1\"))[T.Ghost]        5.7343      8.488      0.676      0.500     -10.960      22.429\n",
       "C(Q(\"Type 1\"))[T.Grass]        3.3275      5.496      0.605      0.545      -7.481      14.136\n",
       "C(Q(\"Type 1\"))[T.Ground]       9.5118      7.076      1.344      0.180      -4.404      23.428\n",
       "C(Q(\"Type 1\"))[T.Ice]         -0.9313      7.717     -0.121      0.904     -16.108      14.246\n",
       "C(Q(\"Type 1\"))[T.Normal]      18.4816      5.312      3.479      0.001       8.034      28.929\n",
       "C(Q(\"Type 1\"))[T.Poison]       8.3411      7.735      1.078      0.282      -6.871      23.554\n",
       "C(Q(\"Type 1\"))[T.Psychic]      1.8061      6.164      0.293      0.770     -10.317      13.930\n",
       "C(Q(\"Type 1\"))[T.Rock]        -3.8558      6.503     -0.593      0.554     -16.645       8.933\n",
       "C(Q(\"Type 1\"))[T.Steel]       -4.0053      8.044     -0.498      0.619     -19.826      11.816\n",
       "C(Q(\"Type 1\"))[T.Water]        9.7988      5.166      1.897      0.059      -0.361      19.959\n",
       "C(Q(\"Type 2\"))[T.Dark]         5.8719     15.185      0.387      0.699     -23.993      35.737\n",
       "C(Q(\"Type 2\"))[T.Dragon]      13.2777     14.895      0.891      0.373     -16.016      42.571\n",
       "C(Q(\"Type 2\"))[T.Electric]    14.3228     17.314      0.827      0.409     -19.728      48.374\n",
       "C(Q(\"Type 2\"))[T.Fairy]        2.8426     14.268      0.199      0.842     -25.218      30.903\n",
       "C(Q(\"Type 2\"))[T.Fighting]     1.9741     14.089      0.140      0.889     -25.735      29.683\n",
       "C(Q(\"Type 2\"))[T.Fire]         0.2001     15.730      0.013      0.990     -30.736      31.136\n",
       "C(Q(\"Type 2\"))[T.Flying]       6.7292     13.581      0.495      0.621     -19.980      33.438\n",
       "C(Q(\"Type 2\"))[T.Ghost]      -10.9402     15.895     -0.688      0.492     -42.201      20.321\n",
       "C(Q(\"Type 2\"))[T.Grass]        2.5119     14.540      0.173      0.863     -26.084      31.108\n",
       "C(Q(\"Type 2\"))[T.Ground]      13.6042     13.655      0.996      0.320     -13.250      40.459\n",
       "C(Q(\"Type 2\"))[T.Ice]         19.7950     15.068      1.314      0.190      -9.840      49.430\n",
       "C(Q(\"Type 2\"))[T.None]         7.6068     13.162      0.578      0.564     -18.279      33.493\n",
       "C(Q(\"Type 2\"))[T.Normal]      17.3191     17.764      0.975      0.330     -17.618      52.256\n",
       "C(Q(\"Type 2\"))[T.Poison]       0.7770     14.575      0.053      0.958     -27.887      29.441\n",
       "C(Q(\"Type 2\"))[T.Psychic]      4.2480     14.174      0.300      0.765     -23.628      32.124\n",
       "C(Q(\"Type 2\"))[T.Rock]         6.8858     16.221      0.424      0.671     -25.017      38.788\n",
       "C(Q(\"Type 2\"))[T.Steel]      -11.9623     14.973     -0.799      0.425     -41.409      17.485\n",
       "C(Q(\"Type 2\"))[T.Water]        5.8097     14.763      0.394      0.694     -23.225      34.845\n",
       "Attack                         0.2508      0.051      4.940      0.000       0.151       0.351\n",
       "Defense                       -0.0096      0.060     -0.160      0.873      -0.127       0.108\n",
       "Speed                         -0.1538      0.051     -2.998      0.003      -0.255      -0.053\n",
       "Q(\"Sp. Def\")                   0.3484      0.059      5.936      0.000       0.233       0.464\n",
       "Q(\"Sp. Atk\")                   0.1298      0.051      2.525      0.012       0.029       0.231\n",
       "==============================================================================\n",
       "Omnibus:                      286.476   Durbin-Watson:                   1.917\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5187.327\n",
       "Skew:                           2.807   Prob(JB):                         0.00\n",
       "Kurtosis:                      19.725   Cond. No.                     9.21e+03\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 9.21e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's something a little more reasonable...\n",
    "model5_linear_form = 'HP ~ Attack + Defense + Speed + Legendary'\n",
    "model5_linear_form += ' + Q(\"Sp. Def\") + Q(\"Sp. Atk\")'\n",
    "model5_linear_form += ' + C(Generation) + C(Q(\"Type 1\")) + C(Q(\"Type 2\"))'\n",
    "\n",
    "model5_spec = smf.ols(formula=model5_linear_form, data=pokeaman_train)\n",
    "model5_fit = model5_spec.fit()\n",
    "model5_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a0dc7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.3920134083531893\n",
      "'Out of sample' R-squared: 0.30015614488652215\n"
     ]
    }
   ],
   "source": [
    "yhat_model5 = model5_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model5_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model5)[0,1]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70dc297",
   "metadata": {},
   "source": [
    "Model 5 is an extended version of model 3 and 4 since it uses all of the predictors from the previous models but without the interaction terms, which is better because we can now focus on the main effects instead of interaction between 2 main effects. However, instead of just plain old predictor variables and quoted variables, we add categorical variables (using C()) to look at how our chosen categorical variables also have an effect on HP, which now broadens our scope of factors that influence the outcome. So as an extension from the previous models, we added more predictor variables which are categorical data and removed the interaction terms from model 4 so we only focus on the main effects. Now that we don't have redundant variables, we reduce the chance of overfitting but risk the strength of our predictive model due to the influence the interaction terms might've had on the outcome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eea79d60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.333</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.319</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   24.36</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 13 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>2.25e-30</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:53:04</td>     <th>  Log-Likelihood:    </th> <td> -1783.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   3585.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   391</td>      <th>  BIC:               </th> <td>   3621.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     8</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                   <td></td>                     <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                          <td>   22.8587</td> <td>    3.876</td> <td>    5.897</td> <td> 0.000</td> <td>   15.238</td> <td>   30.479</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(Q(\"Type 1\") == \"Normal\")[T.True]</th> <td>   17.5594</td> <td>    3.339</td> <td>    5.258</td> <td> 0.000</td> <td>   10.994</td> <td>   24.125</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(Q(\"Type 1\") == \"Water\")[T.True]</th>  <td>    9.0301</td> <td>    3.172</td> <td>    2.847</td> <td> 0.005</td> <td>    2.794</td> <td>   15.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(Generation == 2)[T.True]</th>         <td>    6.5293</td> <td>    2.949</td> <td>    2.214</td> <td> 0.027</td> <td>    0.732</td> <td>   12.327</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(Generation == 5)[T.True]</th>         <td>    8.4406</td> <td>    2.711</td> <td>    3.114</td> <td> 0.002</td> <td>    3.112</td> <td>   13.770</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack</th>                             <td>    0.2454</td> <td>    0.037</td> <td>    6.639</td> <td> 0.000</td> <td>    0.173</td> <td>    0.318</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed</th>                              <td>   -0.1370</td> <td>    0.045</td> <td>   -3.028</td> <td> 0.003</td> <td>   -0.226</td> <td>   -0.048</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\")</th>                       <td>    0.3002</td> <td>    0.045</td> <td>    6.662</td> <td> 0.000</td> <td>    0.212</td> <td>    0.389</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Atk\")</th>                       <td>    0.1192</td> <td>    0.042</td> <td>    2.828</td> <td> 0.005</td> <td>    0.036</td> <td>    0.202</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>271.290</td> <th>  Durbin-Watson:     </th> <td>   1.999</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>4238.692</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.651</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>18.040</td>  <th>  Cond. No.          </th> <td>    618.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                     &        HP        & \\textbf{  R-squared:         } &     0.333   \\\\\n",
       "\\textbf{Model:}                             &       OLS        & \\textbf{  Adj. R-squared:    } &     0.319   \\\\\n",
       "\\textbf{Method:}                            &  Least Squares   & \\textbf{  F-statistic:       } &     24.36   \\\\\n",
       "\\textbf{Date:}                              & Wed, 13 Nov 2024 & \\textbf{  Prob (F-statistic):} &  2.25e-30   \\\\\n",
       "\\textbf{Time:}                              &     21:53:04     & \\textbf{  Log-Likelihood:    } &   -1783.6   \\\\\n",
       "\\textbf{No. Observations:}                  &         400      & \\textbf{  AIC:               } &     3585.   \\\\\n",
       "\\textbf{Df Residuals:}                      &         391      & \\textbf{  BIC:               } &     3621.   \\\\\n",
       "\\textbf{Df Model:}                          &           8      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}                   &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                            & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                          &      22.8587  &        3.876     &     5.897  &         0.000        &       15.238    &       30.479     \\\\\n",
       "\\textbf{I(Q(\"Type 1\") == \"Normal\")[T.True]} &      17.5594  &        3.339     &     5.258  &         0.000        &       10.994    &       24.125     \\\\\n",
       "\\textbf{I(Q(\"Type 1\") == \"Water\")[T.True]}  &       9.0301  &        3.172     &     2.847  &         0.005        &        2.794    &       15.266     \\\\\n",
       "\\textbf{I(Generation == 2)[T.True]}         &       6.5293  &        2.949     &     2.214  &         0.027        &        0.732    &       12.327     \\\\\n",
       "\\textbf{I(Generation == 5)[T.True]}         &       8.4406  &        2.711     &     3.114  &         0.002        &        3.112    &       13.770     \\\\\n",
       "\\textbf{Attack}                             &       0.2454  &        0.037     &     6.639  &         0.000        &        0.173    &        0.318     \\\\\n",
       "\\textbf{Speed}                              &      -0.1370  &        0.045     &    -3.028  &         0.003        &       -0.226    &       -0.048     \\\\\n",
       "\\textbf{Q(\"Sp. Def\")}                       &       0.3002  &        0.045     &     6.662  &         0.000        &        0.212    &        0.389     \\\\\n",
       "\\textbf{Q(\"Sp. Atk\")}                       &       0.1192  &        0.042     &     2.828  &         0.005        &        0.036    &        0.202     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 271.290 & \\textbf{  Durbin-Watson:     } &    1.999  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 4238.692  \\\\\n",
       "\\textbf{Skew:}          &   2.651 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  18.040 & \\textbf{  Cond. No.          } &     618.  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.333\n",
       "Model:                            OLS   Adj. R-squared:                  0.319\n",
       "Method:                 Least Squares   F-statistic:                     24.36\n",
       "Date:                Wed, 13 Nov 2024   Prob (F-statistic):           2.25e-30\n",
       "Time:                        21:53:04   Log-Likelihood:                -1783.6\n",
       "No. Observations:                 400   AIC:                             3585.\n",
       "Df Residuals:                     391   BIC:                             3621.\n",
       "Df Model:                           8                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "======================================================================================================\n",
       "                                         coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------------------------\n",
       "Intercept                             22.8587      3.876      5.897      0.000      15.238      30.479\n",
       "I(Q(\"Type 1\") == \"Normal\")[T.True]    17.5594      3.339      5.258      0.000      10.994      24.125\n",
       "I(Q(\"Type 1\") == \"Water\")[T.True]      9.0301      3.172      2.847      0.005       2.794      15.266\n",
       "I(Generation == 2)[T.True]             6.5293      2.949      2.214      0.027       0.732      12.327\n",
       "I(Generation == 5)[T.True]             8.4406      2.711      3.114      0.002       3.112      13.770\n",
       "Attack                                 0.2454      0.037      6.639      0.000       0.173       0.318\n",
       "Speed                                 -0.1370      0.045     -3.028      0.003      -0.226      -0.048\n",
       "Q(\"Sp. Def\")                           0.3002      0.045      6.662      0.000       0.212       0.389\n",
       "Q(\"Sp. Atk\")                           0.1192      0.042      2.828      0.005       0.036       0.202\n",
       "==============================================================================\n",
       "Omnibus:                      271.290   Durbin-Watson:                   1.999\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             4238.692\n",
       "Skew:                           2.651   Prob(JB):                         0.00\n",
       "Kurtosis:                      18.040   Cond. No.                         618.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's something a little more reasonable...\n",
    "model6_linear_form = 'HP ~ Attack + Speed + Q(\"Sp. Def\") + Q(\"Sp. Atk\")'\n",
    "# And here we'll add the significant indicators from the previous model\n",
    "# https://chatgpt.com/share/81ab88df-4f07-49f9-a44a-de0cfd89c67c\n",
    "model6_linear_form += ' + I(Q(\"Type 1\")==\"Normal\")'\n",
    "model6_linear_form += ' + I(Q(\"Type 1\")==\"Water\")'\n",
    "model6_linear_form += ' + I(Generation==2)'\n",
    "model6_linear_form += ' + I(Generation==5)'\n",
    "\n",
    "model6_spec = smf.ols(formula=model6_linear_form, data=pokeaman_train)\n",
    "model6_fit = model6_spec.fit()\n",
    "model6_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d91abe7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.3326310334310908\n",
      "'Out of sample' R-squared: 0.29572460427079933\n"
     ]
    }
   ],
   "source": [
    "yhat_model6 = model6_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model6_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model6)[0,1]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c65936",
   "metadata": {},
   "source": [
    "Model 6 now excludes the categorical variables and adds indicator variables instead to indicate when a specified observation is in Type 1 and Generation. In this model, we're really only interested when the Type 1 is normal or water, and when the Generation of the pokeaman is 2 or 5. In model 5, we used categorical variables that encapsulated all levels inside them like for Type 1 it included grass, water, fire, rock, and for Generation it included 2, 3, 4, etc. In this model, we can specify what we're interested in so we only want to see when Type 1 is water or normal, and Generation when it's 2 or 5. This is a more targeted approach since we're reducing the number of categorical variables by only modeling a subset of our predictors. Model 6 focuses more on whether these specific categories have a significant effect on HP, which can be useful for more targeted hypotheses. We also reduce overfitting by a large margin by only focusing on few categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "210a2353",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.378</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.347</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   12.16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 13 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>4.20e-29</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:53:04</td>     <th>  Log-Likelihood:    </th> <td> -1769.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   3579.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   380</td>      <th>  BIC:               </th> <td>   3659.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    19</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                     <td></td>                       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                              <td>   95.1698</td> <td>   34.781</td> <td>    2.736</td> <td> 0.007</td> <td>   26.783</td> <td>  163.556</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(Q(\"Type 1\") == \"Normal\")[T.True]</th>     <td>   18.3653</td> <td>    3.373</td> <td>    5.445</td> <td> 0.000</td> <td>   11.733</td> <td>   24.997</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(Q(\"Type 1\") == \"Water\")[T.True]</th>      <td>    9.2913</td> <td>    3.140</td> <td>    2.959</td> <td> 0.003</td> <td>    3.117</td> <td>   15.466</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(Generation == 2)[T.True]</th>             <td>    7.0711</td> <td>    2.950</td> <td>    2.397</td> <td> 0.017</td> <td>    1.271</td> <td>   12.871</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(Generation == 5)[T.True]</th>             <td>    7.8557</td> <td>    2.687</td> <td>    2.923</td> <td> 0.004</td> <td>    2.572</td> <td>   13.140</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack</th>                                 <td>   -0.6975</td> <td>    0.458</td> <td>   -1.523</td> <td> 0.129</td> <td>   -1.598</td> <td>    0.203</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed</th>                                  <td>   -1.8147</td> <td>    0.554</td> <td>   -3.274</td> <td> 0.001</td> <td>   -2.905</td> <td>   -0.725</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed</th>                           <td>    0.0189</td> <td>    0.007</td> <td>    2.882</td> <td> 0.004</td> <td>    0.006</td> <td>    0.032</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\")</th>                           <td>   -0.5532</td> <td>    0.546</td> <td>   -1.013</td> <td> 0.312</td> <td>   -1.627</td> <td>    0.521</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Q(\"Sp. Def\")</th>                    <td>    0.0090</td> <td>    0.007</td> <td>    1.311</td> <td> 0.191</td> <td>   -0.004</td> <td>    0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Q(\"Sp. Def\")</th>                     <td>    0.0208</td> <td>    0.008</td> <td>    2.571</td> <td> 0.011</td> <td>    0.005</td> <td>    0.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Q(\"Sp. Def\")</th>              <td>   -0.0002</td> <td> 9.06e-05</td> <td>   -2.277</td> <td> 0.023</td> <td>   -0.000</td> <td>-2.82e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Atk\")</th>                           <td>   -0.7277</td> <td>    0.506</td> <td>   -1.439</td> <td> 0.151</td> <td>   -1.722</td> <td>    0.267</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Q(\"Sp. Atk\")</th>                    <td>    0.0136</td> <td>    0.005</td> <td>    2.682</td> <td> 0.008</td> <td>    0.004</td> <td>    0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Q(\"Sp. Atk\")</th>                     <td>    0.0146</td> <td>    0.007</td> <td>    2.139</td> <td> 0.033</td> <td>    0.001</td> <td>    0.028</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Q(\"Sp. Atk\")</th>              <td>   -0.0002</td> <td>  5.4e-05</td> <td>   -3.383</td> <td> 0.001</td> <td>   -0.000</td> <td>-7.65e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>              <td>    0.0103</td> <td>    0.007</td> <td>    1.516</td> <td> 0.130</td> <td>   -0.003</td> <td>    0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>       <td>   -0.0001</td> <td> 6.71e-05</td> <td>   -2.119</td> <td> 0.035</td> <td>   -0.000</td> <td>-1.03e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>        <td>   -0.0002</td> <td> 8.82e-05</td> <td>   -2.075</td> <td> 0.039</td> <td>   -0.000</td> <td>-9.62e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th> <td>  2.03e-06</td> <td> 7.42e-07</td> <td>    2.734</td> <td> 0.007</td> <td>  5.7e-07</td> <td> 3.49e-06</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>252.300</td> <th>  Durbin-Watson:     </th> <td>   1.953</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>3474.611</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.438</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>16.590</td>  <th>  Cond. No.          </th> <td>2.34e+09</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.34e+09. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                         &        HP        & \\textbf{  R-squared:         } &     0.378   \\\\\n",
       "\\textbf{Model:}                                 &       OLS        & \\textbf{  Adj. R-squared:    } &     0.347   \\\\\n",
       "\\textbf{Method:}                                &  Least Squares   & \\textbf{  F-statistic:       } &     12.16   \\\\\n",
       "\\textbf{Date:}                                  & Wed, 13 Nov 2024 & \\textbf{  Prob (F-statistic):} &  4.20e-29   \\\\\n",
       "\\textbf{Time:}                                  &     21:53:04     & \\textbf{  Log-Likelihood:    } &   -1769.5   \\\\\n",
       "\\textbf{No. Observations:}                      &         400      & \\textbf{  AIC:               } &     3579.   \\\\\n",
       "\\textbf{Df Residuals:}                          &         380      & \\textbf{  BIC:               } &     3659.   \\\\\n",
       "\\textbf{Df Model:}                              &          19      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}                       &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                                & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                              &      95.1698  &       34.781     &     2.736  &         0.007        &       26.783    &      163.556     \\\\\n",
       "\\textbf{I(Q(\"Type 1\") == \"Normal\")[T.True]}     &      18.3653  &        3.373     &     5.445  &         0.000        &       11.733    &       24.997     \\\\\n",
       "\\textbf{I(Q(\"Type 1\") == \"Water\")[T.True]}      &       9.2913  &        3.140     &     2.959  &         0.003        &        3.117    &       15.466     \\\\\n",
       "\\textbf{I(Generation == 2)[T.True]}             &       7.0711  &        2.950     &     2.397  &         0.017        &        1.271    &       12.871     \\\\\n",
       "\\textbf{I(Generation == 5)[T.True]}             &       7.8557  &        2.687     &     2.923  &         0.004        &        2.572    &       13.140     \\\\\n",
       "\\textbf{Attack}                                 &      -0.6975  &        0.458     &    -1.523  &         0.129        &       -1.598    &        0.203     \\\\\n",
       "\\textbf{Speed}                                  &      -1.8147  &        0.554     &    -3.274  &         0.001        &       -2.905    &       -0.725     \\\\\n",
       "\\textbf{Attack:Speed}                           &       0.0189  &        0.007     &     2.882  &         0.004        &        0.006    &        0.032     \\\\\n",
       "\\textbf{Q(\"Sp. Def\")}                           &      -0.5532  &        0.546     &    -1.013  &         0.312        &       -1.627    &        0.521     \\\\\n",
       "\\textbf{Attack:Q(\"Sp. Def\")}                    &       0.0090  &        0.007     &     1.311  &         0.191        &       -0.004    &        0.023     \\\\\n",
       "\\textbf{Speed:Q(\"Sp. Def\")}                     &       0.0208  &        0.008     &     2.571  &         0.011        &        0.005    &        0.037     \\\\\n",
       "\\textbf{Attack:Speed:Q(\"Sp. Def\")}              &      -0.0002  &     9.06e-05     &    -2.277  &         0.023        &       -0.000    &    -2.82e-05     \\\\\n",
       "\\textbf{Q(\"Sp. Atk\")}                           &      -0.7277  &        0.506     &    -1.439  &         0.151        &       -1.722    &        0.267     \\\\\n",
       "\\textbf{Attack:Q(\"Sp. Atk\")}                    &       0.0136  &        0.005     &     2.682  &         0.008        &        0.004    &        0.024     \\\\\n",
       "\\textbf{Speed:Q(\"Sp. Atk\")}                     &       0.0146  &        0.007     &     2.139  &         0.033        &        0.001    &        0.028     \\\\\n",
       "\\textbf{Attack:Speed:Q(\"Sp. Atk\")}              &      -0.0002  &      5.4e-05     &    -3.383  &         0.001        &       -0.000    &    -7.65e-05     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):Q(\"Sp. Atk\")}              &       0.0103  &        0.007     &     1.516  &         0.130        &       -0.003    &        0.024     \\\\\n",
       "\\textbf{Attack:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}       &      -0.0001  &     6.71e-05     &    -2.119  &         0.035        &       -0.000    &    -1.03e-05     \\\\\n",
       "\\textbf{Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}        &      -0.0002  &     8.82e-05     &    -2.075  &         0.039        &       -0.000    &    -9.62e-06     \\\\\n",
       "\\textbf{Attack:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")} &     2.03e-06  &     7.42e-07     &     2.734  &         0.007        &      5.7e-07    &     3.49e-06     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 252.300 & \\textbf{  Durbin-Watson:     } &    1.953  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 3474.611  \\\\\n",
       "\\textbf{Skew:}          &   2.438 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  16.590 & \\textbf{  Cond. No.          } & 2.34e+09  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 2.34e+09. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.378\n",
       "Model:                            OLS   Adj. R-squared:                  0.347\n",
       "Method:                 Least Squares   F-statistic:                     12.16\n",
       "Date:                Wed, 13 Nov 2024   Prob (F-statistic):           4.20e-29\n",
       "Time:                        21:53:04   Log-Likelihood:                -1769.5\n",
       "No. Observations:                 400   AIC:                             3579.\n",
       "Df Residuals:                     380   BIC:                             3659.\n",
       "Df Model:                          19                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==========================================================================================================\n",
       "                                             coef    std err          t      P>|t|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------------------------\n",
       "Intercept                                 95.1698     34.781      2.736      0.007      26.783     163.556\n",
       "I(Q(\"Type 1\") == \"Normal\")[T.True]        18.3653      3.373      5.445      0.000      11.733      24.997\n",
       "I(Q(\"Type 1\") == \"Water\")[T.True]          9.2913      3.140      2.959      0.003       3.117      15.466\n",
       "I(Generation == 2)[T.True]                 7.0711      2.950      2.397      0.017       1.271      12.871\n",
       "I(Generation == 5)[T.True]                 7.8557      2.687      2.923      0.004       2.572      13.140\n",
       "Attack                                    -0.6975      0.458     -1.523      0.129      -1.598       0.203\n",
       "Speed                                     -1.8147      0.554     -3.274      0.001      -2.905      -0.725\n",
       "Attack:Speed                               0.0189      0.007      2.882      0.004       0.006       0.032\n",
       "Q(\"Sp. Def\")                              -0.5532      0.546     -1.013      0.312      -1.627       0.521\n",
       "Attack:Q(\"Sp. Def\")                        0.0090      0.007      1.311      0.191      -0.004       0.023\n",
       "Speed:Q(\"Sp. Def\")                         0.0208      0.008      2.571      0.011       0.005       0.037\n",
       "Attack:Speed:Q(\"Sp. Def\")                 -0.0002   9.06e-05     -2.277      0.023      -0.000   -2.82e-05\n",
       "Q(\"Sp. Atk\")                              -0.7277      0.506     -1.439      0.151      -1.722       0.267\n",
       "Attack:Q(\"Sp. Atk\")                        0.0136      0.005      2.682      0.008       0.004       0.024\n",
       "Speed:Q(\"Sp. Atk\")                         0.0146      0.007      2.139      0.033       0.001       0.028\n",
       "Attack:Speed:Q(\"Sp. Atk\")                 -0.0002    5.4e-05     -3.383      0.001      -0.000   -7.65e-05\n",
       "Q(\"Sp. Def\"):Q(\"Sp. Atk\")                  0.0103      0.007      1.516      0.130      -0.003       0.024\n",
       "Attack:Q(\"Sp. Def\"):Q(\"Sp. Atk\")          -0.0001   6.71e-05     -2.119      0.035      -0.000   -1.03e-05\n",
       "Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")           -0.0002   8.82e-05     -2.075      0.039      -0.000   -9.62e-06\n",
       "Attack:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")   2.03e-06   7.42e-07      2.734      0.007     5.7e-07    3.49e-06\n",
       "==============================================================================\n",
       "Omnibus:                      252.300   Durbin-Watson:                   1.953\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             3474.611\n",
       "Skew:                           2.438   Prob(JB):                         0.00\n",
       "Kurtosis:                      16.590   Cond. No.                     2.34e+09\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 2.34e+09. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And here's a slight change that seems to perhaps improve prediction...\n",
    "model7_linear_form = 'HP ~ Attack * Speed * Q(\"Sp. Def\") * Q(\"Sp. Atk\")'\n",
    "model7_linear_form += ' + I(Q(\"Type 1\")==\"Normal\")'\n",
    "model7_linear_form += ' + I(Q(\"Type 1\")==\"Water\")'\n",
    "model7_linear_form += ' + I(Generation==2)'\n",
    "model7_linear_form += ' + I(Generation==5)'\n",
    "\n",
    "model7_spec = smf.ols(formula=model7_linear_form, data=pokeaman_train)\n",
    "model7_fit = model7_spec.fit()\n",
    "model7_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3b677e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.37818209127432456\n",
      "'Out of sample' R-squared: 0.35055389205977444\n"
     ]
    }
   ],
   "source": [
    "yhat_model7 = model7_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model7_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model7)[0,1]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11594f6",
   "metadata": {},
   "source": [
    "Model 7 includes what we do in model 6 and 4 by including indicator variables and interaction terms. The interaction terms allow us to capture more complex and perhaps underlying relationships between these variables and their effect on HP (outcome). The reason why indicator variables are better than categorical variables is explained in the last markdown on why model 6 was better than 5, but even though model 7 includes the same indicator variables as model 6, it now has interaction terms as opposed to separate predictor variables. Interaction terms are useful for evaluating the effects of combined variables which is important in the case where one predictor depends on the value of another. While model 7 is basically a copy of model 6, the interaction terms in this model makes it able to capture more hidden relationships between the predictors and the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "feeb1ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>252.300</td> <th>  Durbin-Watson:     </th> <td>   1.953</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>3474.611</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.438</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>16.590</td>  <th>  Cond. No.          </th> <td>    15.4</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Omnibus:}       & 252.300 & \\textbf{  Durbin-Watson:     } &    1.953  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 3474.611  \\\\\n",
       "\\textbf{Skew:}          &   2.438 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  16.590 & \\textbf{  Cond. No.          } &     15.4  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And here's a slight change that seems to perhas improve prediction...\n",
    "model7_linear_form_CS = 'HP ~ scale(center(Attack)) * scale(center(Speed))'\n",
    "model7_linear_form_CS += ' * scale(center(Q(\"Sp. Def\"))) * scale(center(Q(\"Sp. Atk\")))'\n",
    "# We DO NOT center and scale indicator variables\n",
    "model7_linear_form_CS += ' + I(Q(\"Type 1\")==\"Normal\")'\n",
    "model7_linear_form_CS += ' + I(Q(\"Type 1\")==\"Water\")'\n",
    "model7_linear_form_CS += ' + I(Generation==2)'\n",
    "model7_linear_form_CS += ' + I(Generation==5)'\n",
    "\n",
    "model7_CS_spec = smf.ols(formula=model7_linear_form_CS, data=pokeaman_train)\n",
    "model7_CS_fit = model7_CS_spec.fit()\n",
    "model7_CS_fit.summary().tables[-1] \n",
    "# \"Cond. No.\" is NOW 15.4 due to centering and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "821a61e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>252.300</td> <th>  Durbin-Watson:     </th> <td>   1.953</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>3474.611</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.438</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>16.590</td>  <th>  Cond. No.          </th> <td>2.34e+09</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Omnibus:}       & 252.300 & \\textbf{  Durbin-Watson:     } &    1.953  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 3474.611  \\\\\n",
       "\\textbf{Skew:}          &   2.438 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  16.590 & \\textbf{  Cond. No.          } & 2.34e+09  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"Cond. No.\" WAS 2,340,000,000 WITHOUT to centering and scaling\n",
    "model7_fit.summary().tables[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481023de",
   "metadata": {},
   "source": [
    "In this slightly modified form of model 7, we include every aspect from that model except that we modify the interaction terms so that they're all being evaluated on the same scale. *scale()* transforms the variable to have a mean of 0 and standard deviation of 1, while *center()* subtracts the mean from the variable, ensuring it's centered around 0. By standardizing the variables, we make sure that all continuous predictors are on the same scale, which improves stability and reduces the risk that one variable may be on a completely different scale than another. Standardizing also makes the interpretation of the data easier since they're all now centered around 0 and the model coefficients represent the changes in the outcome for one standard deviation change in the predictors. This also translates to how one STD change in something like Attack interacts with one STD change in Speed on HP. Before standardizing, the interaction terms might've produced coefficients that were difficult to interpret as each predictor might've been in different units and ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbbf01d",
   "metadata": {},
   "source": [
    "### 8. Work with a ChatBot to write a *for* loop to create, collect, and visualize many different paired \"in sample\" and \"out of sample\" *model performance* metric actualizations (by not using *np.random.seed(130)* within each loop iteration); and explain in your own words the meaning of your results and purpose of this demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9051a5b1",
   "metadata": {},
   "source": [
    "When visualizing the many different paired in-sample and out-of-sample $R^2$ of model 3, we see that many of the data points are clustered around a specific area. The y=x line represents the ideal performance where the in-sample and out-of-sample $R^2$ are equal which means that the training model predicts perfectly. Data points that fall below the line means that the model underfits the data while points over the line means that the model overfits the data.This graph shows how well the model performs and assesses if it under or overfits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fcf3c2ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px \n",
    "import plotly.graph_objects as go\n",
    " \n",
    "fifty_fifty = int(pokeaman.shape[0]*0.5)\n",
    "pokeaman_train,pokeaman_test = train_test_split(pokeaman, train_size=fifty_fifty)\n",
    "model3_linear_form = 'HP ~ Attack + Defense'\n",
    "\n",
    "reps = 100\n",
    "in_sample_Rsquared = np.array([0.0]*reps)\n",
    "out_of_sample_Rsquared = np.array([0.0]*reps)\n",
    "for i in range(reps):\n",
    "    pokeaman_train,pokeaman_test = train_test_split(pokeaman, train_size=fifty_fifty)\n",
    "    final_model_fit = smf.ols(formula=model3_linear_form, \n",
    "                              data=pokeaman_train).fit()\n",
    "    in_sample_Rsquared[i] = final_model_fit.rsquared\n",
    "    out_of_sample_Rsquared[i] = \\\n",
    "      np.corrcoef(pokeaman_test.HP, \n",
    "                  final_model_fit.predict(pokeaman_test))[0,1]**2\n",
    "    \n",
    "df = pd.DataFrame({\"In Sample Performance (Rsquared)\": in_sample_Rsquared,\n",
    "                   \"Out of Sample Performance (Rsquared)\": out_of_sample_Rsquared})     \n",
    "fig = px.scatter(df, x=\"In Sample Performance (Rsquared)\", \n",
    "                     y=\"Out of Sample Performance (Rsquared)\")\n",
    "fig.add_trace(go.Scatter(x=[0,1], y=[0,1], name=\"y=x\", line_shape='linear'))\n",
    "fig.show(renderer=\"png\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0145e1",
   "metadata": {},
   "source": [
    "### 9. Work with a ChatBot to understand the meaning of the illustration below; and, explain this in your own words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "41d57caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.37818209127432456 (original)\n",
      "'Out of sample' R-squared: 0.006328729000715909 (original)\n",
      "'In sample' R-squared:     0.5726118179916575 (gen1_predict_future)\n",
      "'Out of sample' R-squared: 0.11151363354803218 (gen1_predict_future)\n"
     ]
    }
   ],
   "source": [
    "model7_gen1_predict_future = smf.ols(formula=model7_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation==1])\n",
    "model7_gen1_predict_future_fit = model7_gen1_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model7_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model7)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model7_gen1_predict_future_fit.rsquared, \"(gen1_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation!=1].HP\n",
    "yhat = model7_gen1_predict_future_fit.predict(pokeaman[pokeaman.Generation!=1])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1_predict_future)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285ba04f",
   "metadata": {},
   "source": [
    "We are using model 7 and training only generation 1 pokeaman, then fitting the regression model. Now we're calculating the proportion of variance in HP that is explained by the predictors in the original regression for model 7 (in-sample), and then we calculate the out-of-sample $R^2$ which tells how well the model works with new unseen data. A value closer to 0 indicates that the model doesn't do well against unseen data which is the case here. Then we calculate the proportion of variance in HP that is explained by the predictors in the revised model 7 which only contains pokeaman in generation 1. The out-of-sample $R^2$ for this model tells us how well the generation 1 model predicts the HP values for pokeaman not from generation 1. The reason we do this is to see how well a model that's only seen generation 1 data can predict data that's not in generation 1. So the out-of-sample $R^2$ tells us that this model that's only trained on generation 1 data can explain only 10% of the variance in HP for pokeaman from other generations. The reason why we compare these models and wring out a variable that we think might influence the outcome more is to see how sufficient, for example, the generation 1 model alone can predict HP in other generations. The original model 7 only provides insight on how well a multi-generational model with adjustments to generations 2 and 5 generalizes to pokeaman in other generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "628daf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.37818209127432456 (original)\n",
      "'Out of sample' R-squared: 0.006328729000715909 (original)\n",
      "'In sample' R-squared:     0.3904756578094535 (gen1to5_predict_future)\n",
      "'Out of sample' R-squared: 0.23394915464343125 (gen1to5_predict_future)\n"
     ]
    }
   ],
   "source": [
    "model7_gen1to5_predict_future = smf.ols(formula=model7_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation!=6])\n",
    "model7_gen1to5_predict_future_fit = model7_gen1to5_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model7_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model7)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model7_gen1to5_predict_future_fit.rsquared, \"(gen1to5_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation==6].HP\n",
    "yhat = model7_gen1to5_predict_future_fit.predict(pokeaman[pokeaman.Generation==6])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1to5_predict_future)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0811e32",
   "metadata": {},
   "source": [
    "Essentially the same as above, but now we're seeing how well generation 1-5 pokeaman can predict HP values in generation 6. Our out-of-sample $R^2$ tells us that pokeaman of generations 1-5 can only predict around 23% of the HP values in generation 6. The $R^2$ value is the result of how well the generation 1-5 model patterns apply to generation 6 pokeaman. So the low value means that generation 6 may have different characteristics and patterns influencing HP that the generation 1-5 data doesn't have, so the model won't capture the unseen patterns well. To note, the in-sample $R^2$ tells us how well the model fits the data it was trained on so we can tell that the generation 1-5 data isn't too well fitted on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c52c848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.3326310334310908 (original)\n",
      "'Out of sample' R-squared: 0.008035767164905356 (original)\n",
      "'In sample' R-squared:     0.4433880517727282 (gen1_predict_future)\n",
      "'Out of sample' R-squared: 0.1932858534276128 (gen1_predict_future)\n"
     ]
    }
   ],
   "source": [
    "model6_gen1_predict_future = smf.ols(formula=model6_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation==1])\n",
    "model6_gen1_predict_future_fit = model6_gen1_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model6_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model6)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model6_gen1_predict_future_fit.rsquared, \"(gen1_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation!=1].HP\n",
    "yhat = model6_gen1_predict_future_fit.predict(pokeaman[pokeaman.Generation!=1])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1_predict_future)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09946af0",
   "metadata": {},
   "source": [
    "This time, we're using model 6 to evaluate the same generations as in model 7. Compared to model 7 which used both indicator and interaction terms, model 6 uses only indicator and main effect variables. So in this model, we don't evaluate how the interaction of 2 main effects influence the outcome variable. We can compare with the model 7 generation 1's out-of-sample $R^2$ that this value is higher by around 8%, potentially indicating that the interaction terms are irrelevant when considering what *influences* the HP. The in-sample $R^2$ is smaller without the interaction terms, which indicates that they might've been an important factor in *explaining* HP. So the reason why out-of-sample $R^2$ is higher in this rendition of model 6 may be because model 7 is overfitting, crowding the model with irrelevant interaction terms, or just doens't provide enough explanatory power to justify the added complexity of interaction terms. So a simpler model like this one that just considers the main effects may be better at capturing the more important relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6ea14cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.3326310334310908 (original)\n",
      "'Out of sample' R-squared: 0.008035767164905356 (original)\n",
      "'In sample' R-squared:     0.33517279824114776 (gen1to5_predict_future)\n",
      "'Out of sample' R-squared: 0.26262690178799936 (gen1to5_predict_future)\n"
     ]
    }
   ],
   "source": [
    "model6_gen1to5_predict_future = smf.ols(formula=model6_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation!=6])\n",
    "model6_gen1to5_predict_future_fit = model6_gen1to5_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model6_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model6)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model6_gen1to5_predict_future_fit.rsquared, \"(gen1to5_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation==6].HP\n",
    "yhat = model6_gen1to5_predict_future_fit.predict(pokeaman[pokeaman.Generation==6])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1to5_predict_future)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2409b5ed",
   "metadata": {},
   "source": [
    "This compares pokeaman from generations 1-5 using model 6, which again, is exactly like model 7 except it excludes the interaction terms which makes the model more simpler. We're seeing how well generation 1-5 pokeaman can predict the HP in generation 6 pokeaman. Once again, our in-sample $R^2$ for the predicting model 6 is lower than the model 7's which may indicate that model 7 uses too many variables or not the right ones that may be inflating the $R^2$ value. However, a greater out-of-sample $R^2$ for model 6 shows that the interaction terms don't contribute any real power to the predictive model. Models that only look at main effects often generalize better because they don't focus on interactions between main effects. To summarize, I'm saying that using model 6's less complex linear form, we actually find that without the interaction terms, models that use specific generations to predict others like using generation 1-5 pokeaman with model 6's indicators actually generalizes better to other generations like 6 without the need of interaction terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28f2c1d",
   "metadata": {},
   "source": [
    "## Chatbot Links and Summaries\n",
    "### Question 1-2\n",
    "\n",
    "(https://chatgpt.com/share/67310854-d024-8002-8682-e71c5c7f24e6)\n",
    "\n",
    "1. **Multiple Linear Regression with Continuous vs Categorical Predictors**:\n",
    "   - **Continuous predictors** (e.g., amount spent on TV ads and online ads) are evaluated based on their individual effects on the outcome while controlling for the other predictors.\n",
    "   - **Categorical predictors** (e.g., advertising budgets categorized as \"high\" or \"low\") are treated as binary variables in the regression model, where each category is encoded as 0 or 1.\n",
    "\n",
    "2. **Interaction Terms**:\n",
    "   - When predictors interact, an **interaction term** is included in the model to capture how the effect of one predictor depends on the level of another.\n",
    "   - Without an interaction term, each predictor's effect is assumed to be independent of the other.\n",
    "\n",
    "3. **Regression Model with Binary Variables**:\n",
    "   - If TV and online ad budgets are categorized as \"high\" or \"low\", each predictor is encoded as a binary variable (1 = high, 0 = low).\n",
    "   - The model with interaction looks like this: \n",
    "     \\[\n",
    "     $Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 (X_1 \\times X_2) + \\epsilon$\n",
    "     \\]\n",
    "     where \\( X_1 \\) and \\( X_2 \\) are binary variables for TV and online ad budgets.\n",
    "\n",
    "4. **Prediction Examples**:\n",
    "   - **No advertising or both low budgets**: \\( Y = \\beta_0 \\) (baseline).\n",
    "   - **High TV, low online**: \\( Y = \\beta_0 + \\beta_1 \\).\n",
    "   - **Low TV, high online**: \\( Y = \\beta_0 + \\beta_2 \\).\n",
    "   - **Both high budgets**: \\( Y = \\beta_0 + \\beta_1 + \\beta_2 + \\beta_3 \\).\n",
    "   - The **interaction term** \\( \\beta_3 \\) adjusts the combined effect of high budgets, reflecting whether their effect is more or less than the sum of their individual effects.\n",
    "\n",
    "### Question 3\n",
    "\n",
    "(https://chatgpt.com/share/6735521d-0480-8002-8301-df6562cc91e2)\n",
    "\n",
    "1. **Logistic Regression and Visualization**: You asked about visualizing logistic regression results as though they were multivariate linear regression, using Plotly to simulate fitted lines for both a binary and continuous predictor variable.\n",
    "\n",
    "2. **Error Explanation**: We discussed various steps in data visualization and model fitting, and you requested a code modification to visualize the data with \"best fit\" lines for logistic regression, with random noise added for the predictor variables.\n",
    "\n",
    "3. **Code Adjustments**:\n",
    "   - I guided you through a Python code snippet where random noise was added to the predictor variables, and the logistic regression model was fit as if it were a linear regression to plot continuous lines.\n",
    "   - The code visualizes the logistic regression model's fitted coefficients, treating them like a multivariate linear regression, and adds noise for a more realistic appearance of the data.\n",
    "   - We focused on visualizing the relationships between the predictor variables (`WELLNESS_self_rated_mental_health` and `LONELY_others_aware`) and the outcome variable (`feeling_left_out`), plotting the data and regression lines separately for each category of predictors.\n",
    "\n",
    "4. **Output**:\n",
    "   - The result is a Plotly graph showing the noisy data and lines of best fit for different combinations of the binary and continuous predictors.\n",
    "\n",
    "### Question 4\n",
    "\n",
    "(https://chatgpt.com/share/67342d81-d920-8002-ab4e-80fc6403ab7b)\n",
    "\n",
    "1. **Low \\( R^2 \\) and Significant Predictors**: We discussed that a low \\( R^2 \\) with significant p-values for predictors suggests the model does not explain much variance in the outcome, even though the predictors have some statistically significant relationships with it. This can happen if the predictors' effects are small, if there’s a large sample size, or if important variables are missing.\n",
    "\n",
    "2. **Standard Errors Assumptions**: Standard errors assume that error terms are homoscedastic (constant variance) and uncorrelated. Violating these assumptions can lead to biased standard errors, affecting p-values and inference. Robust standard errors can be used to address violations.\n",
    "\n",
    "3. **Multicollinearity**: A high condition number indicates multicollinearity, meaning predictors are highly correlated with each other, leading to instability in coefficient estimates. Multicollinearity affects interpretation rather than directly lowering \\( R^2 \\).\n",
    "\n",
    "4. **Strong Multicollinearity with Low \\( R^2 \\)**: This unusual combination suggests that the predictors are redundant but collectively poor at explaining the outcome. It indicates that other, more relevant predictors might be missing, and that the model specification might need adjustment. \n",
    "\n",
    "### Question 5-6\n",
    "\n",
    "(https://chatgpt.com/share/67343d59-989c-8002-9159-c4e44f60ea31)\n",
    "\n",
    "1. **Training and Test Sets**: We discussed how training and test sets are used in machine learning. The training set is used to train the model, while the test set evaluates how well the model generalizes to new, unseen data. The test set helps identify if the model is overfitting (memorizing the training data) or underfitting (performing poorly on both sets).\n",
    "\n",
    "2. **Linear Regression Code**: We examined a piece of code that creates and fits a linear regression model using `statsmodels`. This code defines a model with `HP` as the target variable, using predictors like `Attack` and `Defense`, with their interactions included. The code then calculates in-sample and out-of-sample R-squared values to evaluate the model's performance.\n",
    "\n",
    "3. **Design Matrix and Multicollinearity**: We explored how the formula in the regression model generates the design matrix (`model4_spec.exog`), which is the numerical representation of the predictors. Interaction terms and transformations in the formula create new predictor variables, which are used to fit the model. We also discussed **multicollinearity**, where highly correlated predictors in the design matrix reduce the stability of the model, leading to overfitting and poor generalization to out-of-sample data.\n",
    "\n",
    "4. **Generalization Issues**: Multicollinearity can lead to poor model generalization because the model struggles to separate the effects of correlated predictors, making it overly sensitive to training data and less reliable when applied to new data.\n",
    "\n",
    "### Question 7\n",
    "\n",
    "(https://chatgpt.com/share/6734496a-1f30-8002-b7ad-4f61f36078ba)\n",
    "\n",
    "#### Model 3 and Model 4:\n",
    "- **Model 3** is a simple multiple linear regression model with predictors `Attack` and `Defense` to explain the dependent variable `HP`.\n",
    "- **Model 4** is an extended version of Model 3, incorporating interaction terms between `Attack`, `Defense`, `Speed`, `Legendary`, `Sp. Def`, and `Sp. Atk`. This allows for more complex relationships between these variables and `HP`.\n",
    "\n",
    "#### Model 5:\n",
    "- **Model 5** further extends **Model 4** by adding more predictors, including continuous variables like `Speed` and `Sp. Def`, as well as categorical variables like `Generation`, `Type 1`, and `Type 2`. This makes the model more comprehensive by accounting for both continuous and categorical factors.\n",
    "\n",
    "#### Model 6:\n",
    "- **Model 6** narrows down the categorical variables (`Type 1` and `Generation`) using binary indicator variables (e.g., \"Normal\" and \"Water\" for `Type 1`, and Generation 2 and Generation 5 for `Generation`).\n",
    "- This is a more focused version compared to Model 5, where all categories of `Type 1` and `Generation` were modeled.\n",
    "\n",
    "#### Model 7:\n",
    "- **Model 7** combines everything: it includes the binary indicator variables for `Type 1` and `Generation`, but it also adds **interaction terms** between `Attack`, `Speed`, `Sp. Def`, and `Sp. Atk`. These interactions allow the model to capture more complex relationships between these variables.\n",
    "- It's a comprehensive model, combining both categorical and continuous factors, along with their interactions.\n",
    "\n",
    "#### Model 7 (CS):\n",
    "- **Model 7 (CS)** replaces the original interaction terms with **scaled and centered** variables. Scaling and centering standardize the continuous variables so that their coefficients can be interpreted on the same scale (in terms of standard deviations).\n",
    "- The interaction terms now model relationships between these standardized variables (`Attack`, `Speed`, `Sp. Def`, and `Sp. Atk`), making the model coefficients easier to interpret.\n",
    "- Indicator variables for `Type 1` and `Generation` remain, allowing the model to capture categorical effects.\n",
    "\n",
    "#### Key Takeaways:\n",
    "- **Model 5** and **Model 6** focus on different ways of handling categorical variables (full categories vs. binary indicators).\n",
    "- **Model 7** builds on these models by adding interaction terms, making the model more complex and capable of capturing higher-order relationships.\n",
    "- **Model 7 (CS)** refines this further by scaling and centering continuous variables, making the coefficients more interpretable and comparable.\n",
    "\n",
    "### Question 8\n",
    "\n",
    "(https://chatgpt.com/share/67352019-ec78-8002-b23c-e3c39471bb1b)\n",
    "\n",
    "1. **Alias `go` in Plotly**: We discussed how `go` is commonly used as an alias for `plotly.graph_objects`, allowing for detailed and customizable visualizations.\n",
    "\n",
    "2. **\\( R^2 \\) as a Model Performance Metric**: We talked about \\( R^2 \\) as a measure of model performance, particularly in regression, and how it indicates the proportion of variance explained by the model.\n",
    "\n",
    "3. **Visualizing \\( R^2 \\) Across Models**: I described what a graph might look like when visualizing in-sample and out-of-sample \\( R^2 \\) values across different model iterations. Typically, this would involve two lines showing the in-sample and out-of-sample \\( R^2 \\) trends.\n",
    "\n",
    "4. **Analyzing Code for \\( R^2 \\) Comparison**: We reviewed code that trains a regression model on different train-test splits of a dataset and visualizes in-sample vs. out-of-sample \\( R^2 \\) values. The resulting scatter plot shows how well the model generalizes by comparing these values over 100 iterations, with a reference line (`y = x`) indicating ideal performance.\n",
    "\n",
    "### Question 9\n",
    "\n",
    "(https://chatgpt.com/share/673541fe-4ca0-8002-8273-ec58dd9a154c)\n",
    "\n",
    "1. **Model 7 Overview**: We looked at model 7, which includes multiple interaction terms among `Attack`, `Speed`, `Sp. Def`, and `Sp. Atk`, as well as indicator variables for Pokémon types and generations. This model is more complex due to the interactions, aiming to capture detailed relationships.\n",
    "\n",
    "2. **Model 6 Overview**: Model 6, in contrast, excludes interaction terms and only includes additive effects. It’s a simpler model designed to capture the main effects of each predictor on `HP`.\n",
    "\n",
    "3. **Generalization Testing with Different Generations**:\n",
    "   - You evaluated how well both models predict HP across generations, particularly focusing on training on Generations 1 to 5 and predicting Generation 6.\n",
    "   - **Model 6** often showed higher out-of-sample R-squared values when applied to new generations, suggesting that its simpler, additive form generalized better than the more complex Model 7.\n",
    "\n",
    "4. **Implications of Higher R-squared in Model 6**:\n",
    "   - Model 6’s better performance in out-of-sample predictions indicates that the interaction terms in Model 7 might be unnecessary, as they didn’t significantly improve predictive power across generations.\n",
    "   - This suggests that the relationships between `Attack`, `Speed`, `Sp. Def`, and `Sp. Atk` with `HP` are effectively captured by additive effects without needing interactions.\n",
    "\n",
    "5. **Conclusion on Overfitting and Model Complexity**:\n",
    "   - The simpler model (Model 6) demonstrated better generalization, implying that Model 7’s added complexity may lead to overfitting rather than meaningful improvement.\n",
    "   - Thus, Model 6 appears more robust and generalizable for predicting across different generations, making it the preferred model for this dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
